{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crytpo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisgraham333/Bitcoin_prediction/blob/main/Crytpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjEtUcv-fXYy"
      },
      "source": [
        "# Crypto prediction model creation\n",
        "*This notebook is created to build a model to predict crypto prices using a range of sources. This is an experiment to see whether this is possible, or whether prices follow a truly random walk. Data sources include:\n",
        "- Previous Bitcoin transaction and price data (using Kraken)\n",
        "- Google Trends data\n",
        "- Activity among other Bitcoin users (from Blockcypher)*\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g9VjqIEFS2L"
      },
      "source": [
        "Things to do:\n",
        "1. Add in bitcoin and other keyword in the news\n",
        "2. Look at whales by pulling from the blockchain and taking the number of transactions over a certain amount \n",
        "3. Look into other factors from https://www.coinspeaker.com/guides/how-big-news-influence-bitcoin-price/ and https://www.forbes.com/sites/billybambrough/2020/03/22/heres-how-to-predict-major-moves-in-the-price-of-bitcoin/#90adc4d7a4ab\n",
        "4. Improve the buy and sell metric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6cJ_xKbgglr"
      },
      "source": [
        "## Chapter 1: Prepare the script\n",
        "Import packages, and set up a link to Google Drive, where the data will be stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMKVXpaegs0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e007a3e4-1139-4af1-e589-c0e988156956"
      },
      "source": [
        "###Install and import packages\n",
        "#Basics\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from functools import reduce\n",
        "import scipy.stats  as stats\n",
        "import gc\n",
        "import sys\n",
        "import os\n",
        "\n",
        "#For pydrive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive \n",
        "from google.colab import auth \n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#For Kraken API\n",
        "import requests\n",
        "import json\n",
        "\n",
        "#For Google trends\n",
        "import datetime\n",
        "!pip install pytrends\n",
        "from pytrends.request import TrendReq \n",
        "\n",
        "#For blockchain\n",
        "!pip install --default-timeout=100 blockcypher\n",
        "from blockcypher import get_address_overview\n",
        "from blockcypher import get_address_details\n",
        "from blockcypher import get_address_full\n",
        "\n",
        "#For Sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.utils import class_weight #for overweighting users\n",
        "from sklearn.metrics import SCORERS\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "\n",
        "#Tensorflow\n",
        "!pip install tensorflow-determinism\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, LSTM, Input, Masking\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "#Set seed\n",
        "seed_object = 7140\n",
        "os.environ['PYTHONHASHSEED']=str(seed_object)\n",
        "os.environ['TF_DETERMINISTIC_OPS']=str(seed_object)\n",
        "random.seed(seed_object)\n",
        "np.random.seed(seed_object)\n",
        "set_seed(seed_object)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytrends\n",
            "  Downloading https://files.pythonhosted.org/packages/96/53/a4a74c33bfdbe1740183e00769377352072e64182913562daf9f5e4f1938/pytrends-4.7.3-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.6/dist-packages (from pytrends) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytrends) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pytrends) (4.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->pytrends) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->pytrends) (1.19.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.25->pytrends) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytrends) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25->pytrends) (1.15.0)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.7.3\n",
            "Collecting blockcypher\n",
            "  Downloading https://files.pythonhosted.org/packages/07/6d/2daa51b4f71b5d7945486d4d977d9bf566db97b092bc9ce0ff35e8fadaeb/blockcypher-1.0.90-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from blockcypher) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0 in /usr/local/lib/python3.6/dist-packages (from blockcypher) (2.8.1)\n",
            "Collecting bitcoin==1.1.39\n",
            "  Downloading https://files.pythonhosted.org/packages/12/ef/569ad753ccc46d483aa3fcfcd0699a7709723ba2fee031e02474deaffa82/bitcoin-1.1.39.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->blockcypher) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->blockcypher) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->blockcypher) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->blockcypher) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0->blockcypher) (1.15.0)\n",
            "Building wheels for collected packages: bitcoin\n",
            "  Building wheel for bitcoin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitcoin: filename=bitcoin-1.1.39-cp36-none-any.whl size=28433 sha256=1b0a635835f44742d0883b1c4df2366796ef6a9d44bd548711c580b4a7c64e8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/ae/e0/80053298b6540fe80388e5e8919d92804ca8a21d0b211655b5\n",
            "Successfully built bitcoin\n",
            "Installing collected packages: bitcoin, blockcypher\n",
            "Successfully installed bitcoin-1.1.39 blockcypher-1.0.90\n",
            "Collecting tensorflow-determinism\n",
            "  Downloading https://files.pythonhosted.org/packages/76/56/79d74f25b326d8719753172496abc524980fa67d1d98bb247021376e370a/tensorflow-determinism-0.3.0.tar.gz\n",
            "Building wheels for collected packages: tensorflow-determinism\n",
            "  Building wheel for tensorflow-determinism (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-determinism: filename=tensorflow_determinism-0.3.0-cp36-none-any.whl size=9159 sha256=9c2ad60e6e31664e1bb47ea93caf82cba3d5e44807bbd8511afdbc4baa2989e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/c3/18/13959a90d3e0d10182a99866d6ff4d0119e9daed6ce014b54c\n",
            "Successfully built tensorflow-determinism\n",
            "Installing collected packages: tensorflow-determinism\n",
            "Successfully installed tensorflow-determinism-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJo0Psz6EiRy"
      },
      "source": [
        "#Set visualization options\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZo2o6OcGZuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5463946-e3f3-41e0-a435-547e7a0cddb5"
      },
      "source": [
        "#Mount Google Drive (to allow saving)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Brf484w70Q"
      },
      "source": [
        "#Set Google Drive filepath. In this, we need \"Raw Data\", \"Last Date\", \"Cleaned Data\" and \"Models\"\n",
        "my_filepath = \"drive/My Drive/Data Science/Crypto/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckf5pdUtv32n"
      },
      "source": [
        "## Chapter 2: Pull previous transactions and price data\n",
        "Download transactions data from Kraken. This is designed for updating previously downloaded data, but can be adjusted for a first pull"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCqkMWYh8K9G"
      },
      "source": [
        "###Create function for downloading new data\n",
        "def downloading_new_data(pair, pair_adj, start_time, end_time):\n",
        "  #Create objects to be updated for the time and the final dataset\n",
        "  temp_time = start_time\n",
        "  transactions_data = pd.DataFrame(columns = [0,1,2,3,4,5])\n",
        "  printcounter = 0\n",
        "  URL = \"https://api.kraken.com/0/public/Trades\"\n",
        "  #Run a loop to collect data\n",
        "  while int(temp_time)<int(end_time):\n",
        "    #Print the time to keep track\n",
        "    if (printcounter % 100 == 0):\n",
        "      print(\"Time pulled:\", pd.to_datetime(int(temp_time)))\n",
        "      print(\"Time of request:\", pd.to_datetime(time.time()*1000000000))\n",
        "    #Sleep to avoid pulling from the API too much\n",
        "    if printcounter % 5 == 0:\n",
        "      time.sleep(15)\n",
        "    #Pull transactions data (Try 3 times)\n",
        "    tries = 3\n",
        "    for i in range(tries):\n",
        "      try:\n",
        "          transactions_data_temp = json.loads(requests.get(URL, params = {\"pair\": pair,\n",
        "                                                      \"since\": temp_time}).content)\n",
        "          transactions_data_temp = pd.DataFrame.from_dict(transactions_data_temp['result'][pair_adj])\n",
        "      except (KeyError, ValueError) as e:\n",
        "          if i < tries - 1: # i is zero indexed\n",
        "              print(\"Retrying\")\n",
        "              continue\n",
        "          else:\n",
        "              print(\"Failed\")\n",
        "              raise\n",
        "      break\n",
        "    #Update objects\n",
        "    transactions_data = transactions_data.append(transactions_data_temp)\n",
        "    temp_time = str(format(transactions_data_temp[2].max()*1000000000, '.0f'))\n",
        "    printcounter += 1\n",
        "  \n",
        "  #Remove objects\n",
        "  del(transactions_data_temp, printcounter)\n",
        "\n",
        "  #Return object\n",
        "  return(transactions_data, temp_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-53xF_X4TKfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7605a97-7850-4af3-e2bb-4ef61827aa8b"
      },
      "source": [
        "###Prepare and pull the data for bitcoin (change last data to the first date interested in if this is the first pull)\n",
        "#Pull the start and end times (end time is current - if this fails do this in batches)\n",
        "last_date_bitcoin = np.loadtxt(my_filepath + \"Last Date/Last_Date_Bitcoin.txt\")\n",
        "start_time_bitcoin = str(format(last_date_bitcoin,'.0f'))\n",
        "end_time = str(format(time.time()*1000000000, '.0f'))\n",
        "##Pull the new data\n",
        "new_data_bitcoin, final_time_bitcoin = downloading_new_data(\"XBTEUR\", \"XXBTZEUR\", start_time_bitcoin, end_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time pulled: 2020-12-26 16:27:10.300000\n",
            "Time of request: 2021-01-06 13:16:52.032999936\n",
            "Time pulled: 2020-12-27 17:29:57.963699968\n",
            "Time of request: 2021-01-06 13:22:26.829432576\n",
            "Time pulled: 2020-12-29 09:45:14.122899968\n",
            "Time of request: 2021-01-06 13:28:01.631375872\n",
            "Time pulled: 2020-12-30 21:01:28.665699840\n",
            "Time of request: 2021-01-06 13:33:43.274811392\n",
            "Time pulled: 2021-01-01 20:36:12.234200064\n",
            "Time of request: 2021-01-06 13:39:25.905658624\n",
            "Time pulled: 2021-01-02 20:54:51.448800\n",
            "Time of request: 2021-01-06 13:45:11.135453952\n",
            "Time pulled: 2021-01-03 15:30:10.384300032\n",
            "Time of request: 2021-01-06 13:50:59.571577600\n",
            "Time pulled: 2021-01-04 10:20:14.366099968\n",
            "Time of request: 2021-01-06 13:56:53.686587392\n",
            "Time pulled: 2021-01-05 09:08:08.078400\n",
            "Time of request: 2021-01-06 14:02:53.700824320\n",
            "Time pulled: 2021-01-06 10:03:23.735300096\n",
            "Time of request: 2021-01-06 14:08:57.347183872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYplF6TSaMdP"
      },
      "source": [
        "###Rename and adjust types\n",
        "new_data_bitcoin.rename(columns={0: 'price', 1: 'volume', 2: 'time', \n",
        "                           3: 'buy_sell', 4: 'market_limit', \n",
        "                           5: 'misc'}, inplace=True)\n",
        "new_data_bitcoin['time'] = pd.to_datetime(new_data_bitcoin['time'],unit='s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExR6xnQOvoy3"
      },
      "source": [
        "###Pull the previous data, append and drop duplicates for bitcoin. If this is the first pull, simply rename new_data_bitcoin as bitcoin_data\n",
        "bitcoin_data = pd.read_csv(my_filepath + \"Raw Data/Raw_Data_Bitcoin.csv\")\n",
        "bitcoin_data = bitcoin_data.append(new_data_bitcoin)\n",
        "new_data_bitcoin = pd.DataFrame()\n",
        "del(new_data_bitcoin)\n",
        "bitcoin_data.drop_duplicates(keep='first', inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce3q3LJ5aXOm"
      },
      "source": [
        "###Save for bitcoin\n",
        "bitcoin_data.to_csv(my_filepath + 'Raw Data/Raw_Data_Bitcoin.csv',index=False)\n",
        "print(final_time_bitcoin,  file=open(my_filepath + 'Last Date/Last_Date_Bitcoin.txt', 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSMwCFK1baux"
      },
      "source": [
        "###Delete bitcoin objects\n",
        "bitcoin_data = pd.DataFrame()\n",
        "del(bitcoin_data, last_date_bitcoin, start_time_bitcoin, final_time_bitcoin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCbB8GnJZ9RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2208fe-744b-462e-9a46-0b88bf7cff9d"
      },
      "source": [
        "###Prepare and pull the data for ethereum (change last data to the first date interested in if this is the first pull)\n",
        "#Pull the start and end times (end time is current - if this fails do this in batches)\n",
        "last_date_ethereum = np.loadtxt(my_filepath + \"Last Date/Last_Date_Ethereum.txt\")\n",
        "start_time_ethereum = str(format(last_date_ethereum,'.0f'))\n",
        "end_time = str(format(time.time()*1000000000, '.0f'))\n",
        "##Pull the new data and save the last date\n",
        "new_data_ethereum, final_time_ethereum = downloading_new_data(\"ETHEUR\", \"XETHZEUR\", start_time_ethereum, end_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time pulled: 2020-09-13 12:59:54.925499904\n",
            "Time of request: 2021-01-06 16:09:39.559493376\n",
            "Time pulled: 2020-09-23 14:25:05.702699776\n",
            "Time of request: 2021-01-06 16:15:17.455465216\n",
            "Time pulled: 2020-10-07 06:23:11.575000064\n",
            "Time of request: 2021-01-06 16:20:55.863287808\n",
            "Time pulled: 2020-10-21 15:53:03.841299968\n",
            "Time of request: 2021-01-06 16:26:41.686530816\n",
            "Time pulled: 2020-11-02 08:26:35.325499904\n",
            "Time of request: 2021-01-06 16:32:29.048775424\n",
            "Time pulled: 2020-11-09 00:21:02.664300032\n",
            "Time of request: 2021-01-06 16:38:18.667533824\n",
            "Time pulled: 2020-11-17 17:06:13.772100096\n",
            "Time of request: 2021-01-06 16:44:13.994258432\n",
            "Time pulled: 2020-11-22 15:02:30.868499968\n",
            "Time of request: 2021-01-06 16:50:14.943987968\n",
            "Time pulled: 2020-11-25 17:42:45.881499904\n",
            "Time of request: 2021-01-06 16:56:12.909421824\n",
            "Time pulled: 2020-11-29 08:25:05.971500032\n",
            "Time of request: 2021-01-06 17:02:16.573541120\n",
            "Time pulled: 2020-12-03 20:41:56.505299968\n",
            "Time of request: 2021-01-06 17:08:25.871072768\n",
            "Time pulled: 2020-12-11 17:22:41.231899904\n",
            "Time of request: 2021-01-06 17:14:39.994460160\n",
            "Time pulled: 2020-12-17 19:48:40.754899968\n",
            "Time of request: 2021-01-06 17:21:00.205482240\n",
            "Time pulled: 2020-12-23 11:30:14.836499968\n",
            "Time of request: 2021-01-06 17:27:22.854722048\n",
            "Time pulled: 2020-12-27 20:34:39.892299776\n",
            "Time of request: 2021-01-06 17:33:48.069457152\n",
            "Time pulled: 2020-12-31 07:16:15.596199936\n",
            "Time of request: 2021-01-06 17:40:18.447581440\n",
            "Time pulled: 2021-01-03 13:47:29.914000128\n",
            "Time of request: 2021-01-06 17:46:50.346805760\n",
            "Time pulled: 2021-01-04 07:44:43.920000\n",
            "Time of request: 2021-01-06 17:53:21.809052416\n",
            "Time pulled: 2021-01-05 01:56:49.939300096\n",
            "Time of request: 2021-01-06 17:59:59.499132928\n",
            "Time pulled: 2021-01-06 16:04:10.558000128\n",
            "Time of request: 2021-01-06 18:06:40.721961216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djAfdwMkjQB8"
      },
      "source": [
        "###Rename and adjust types\n",
        "new_data_ethereum.rename(columns={0: 'price', 1: 'volume', 2: 'time', \n",
        "                           3: 'buy_sell', 4: 'market_limit', \n",
        "                           5: 'misc'}, inplace=True)\n",
        "new_data_ethereum['time'] = pd.to_datetime(new_data_ethereum['time'],unit='s')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oZ9lnW9v_aK"
      },
      "source": [
        "###Pull the previous data, append and drop duplicates for ethereum. If this is the first pull, simply rename new_data_ethereum as ethereum_data\n",
        "ethereum_data = pd.read_csv(my_filepath + \"Raw Data/Raw_Data_Ethereum.csv\")\n",
        "ethereum_data = ethereum_data.append(new_data_ethereum)\n",
        "new_data_ethereum = pd.DataFrame()\n",
        "del(new_data_ethereum)\n",
        "ethereum_data.drop_duplicates(keep='first', inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sECx6O41jYHJ"
      },
      "source": [
        "###Save for ethereum\n",
        "ethereum_data.to_csv(my_filepath + 'Raw Data/Raw_Data_Ethereum.csv',index=False)\n",
        "print(final_time_ethereum,  file=open(my_filepath + 'Last Date/Last_Date_Ethereum.txt', 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZERQ2bOo0PC"
      },
      "source": [
        "###Delete ethereum objects\n",
        "ethereum_data = pd.DataFrame()\n",
        "del(ethereum_data, last_date_ethereum, start_time_ethereum, final_time_ethereum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x69cHw3wEwP"
      },
      "source": [
        "## Chapter 3: Pull google trends data\n",
        "Download hourly trends in overlapping datasets from Google Trends, merge these together, and normalise these. Again, this is designed for updating previously downloaded data, but can be adjusted for a first pull. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9Cs1Z__Tzgr"
      },
      "source": [
        "def downloading_google_trends(year_s, month_s, day_s, year_e, month_e, day_e, keyword, location):\n",
        "  #Specify start and end date, and required keyword\n",
        "  start_date = datetime.date(year_s, month_s, day_s)\n",
        "  end_date = datetime.date(year_e, month_e, day_e)\n",
        "  \n",
        "  #Make a list of weeks\n",
        "  weekly_date_list = []\n",
        "  start_date_temp = start_date\n",
        "  weekly_date_list.append(start_date_temp)\n",
        "  while start_date_temp+datetime.timedelta(days=7) < end_date:\n",
        "      start_date_temp += datetime.timedelta(days=7)\n",
        "      weekly_date_list.append(start_date_temp)\n",
        "  if start_date_temp+datetime.timedelta(days=7) >= end_date:\n",
        "      weekly_date_list.append(end_date)\n",
        "\n",
        "  #Make a list of data for each week, and remove the second of any duplicates (the value for the beginning of next week)\n",
        "  interest_list = []\n",
        "  for i in range(len(weekly_date_list)-1):\n",
        "      p = TrendReq()\n",
        "      keyword_list = [keyword] \n",
        "      interest = p.get_historical_interest(keyword_list, weekly_date_list[i].year, weekly_date_list[i].month, weekly_date_list[i].day, 0,  weekly_date_list[i+1].year, weekly_date_list[i+1].month, weekly_date_list[i+1].day, 0, geo = location, sleep = 1).reset_index()\n",
        "      interest.rename(columns = {'date': \"Date\"}, inplace = True)\n",
        "      interest.drop_duplicates(keep='first', subset = \"Date\", inplace = True)\n",
        "      interest_list.append(interest)\n",
        "\n",
        "  #Rescale the data \n",
        "  interest_list[0][\"Check\"] = interest_list[0][keyword_list[0]]\n",
        "  if len(interest_list) > 1:\n",
        "    ratio_list = []\n",
        "    for i in range(len(interest_list)-1):\n",
        "        #Calculation of the ratio\n",
        "        try:\n",
        "          ratio = float(interest_list[i][keyword_list[0]].iloc[-1])/float(interest_list[i+1][keyword_list[0]].iloc[0]) \n",
        "        except ZeroDivisionError:\n",
        "          ratio = 1\n",
        "        ratio_list.append(ratio)\n",
        "        interest_list[i+1][\"Check\"] = interest_list[i+1][keyword_list[0]].apply(lambda x:x*ratio_list[i])\n",
        "        interest_list[i+1][keyword_list[0]] = interest_list[i+1][\"Check\"]\n",
        "\n",
        "  #Combine the data and return the object\n",
        "  df = pd.concat(interest_list)\n",
        "  df.drop(labels = keyword_list[0] , axis = 1, inplace = True)\n",
        "  df.drop(df[df['isPartial'] == \"True\"].index, inplace=True)\n",
        "  df.drop(labels = \"isPartial\", axis = 1, inplace = True)\n",
        "  df.rename(columns = {'Check': keyword}, inplace = True)\n",
        "  return(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GBsP_rDKoUE"
      },
      "source": [
        "#List the terms, country ISO codes, and country labels to be pulled (i.e. the keywords we want, and the countries/regions that they are being searched from)\n",
        "keyword_list = [\"Bitcoin\", \"Ethereum\", \"Coinbase\"]\n",
        "ISO_codes = [\"\", \"US\", \"GB\", \"JP\", \"IN\"]\n",
        "country_labels = [\"World\", \"US\", \"GB\", \"Japan\", \"India\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ2Zjc0BrohH"
      },
      "source": [
        "# #Start collecting data if we don't have any already (don't run if this isn't the first pull)\n",
        "# Google_Trends_List = []\n",
        "# for i in range(len(ISO_codes)): \n",
        "#   print(country_labels[i])\n",
        "#   for j in range(len(keyword_list)): \n",
        "#     Google_Trends_Temp = downloading_google_trends(2015, 1, 1, 2015, 1, 15, keyword_list[j], ISO_codes[i])\n",
        "#     Google_Trends_Temp = Google_Trends_Temp.add_prefix(country_labels[i] + \"_\")\n",
        "#     Google_Trends_Temp.columns.values[0] = \"Date\"\n",
        "#     Google_Trends_Temp.drop_duplicates(keep='first', subset = \"Date\", inplace = True)\n",
        "#     Google_Trends_List.append(Google_Trends_Temp)\n",
        "# \n",
        "# #Merge together\n",
        "# Google_Trends = reduce(lambda x, y: pd.merge(x, y, on = 'Date'), Google_Trends_List)\n",
        "# \n",
        "# #Capture last date\n",
        "# last_date = Google_Trends['Date'].iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOMCJXAhnBMU"
      },
      "source": [
        "#Read in previous data, and capture the last date, if we have it (don't run if this is the first pull)\n",
        "Google_Trends = pd.read_csv(my_filepath + \"Raw Data/Google_Trends.csv\")\n",
        "last_date = pd.to_datetime(Google_Trends['Date'].iloc[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-2daavlhNyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82efac04-dc9e-4123-95a8-754d551bf46e"
      },
      "source": [
        "#Pull new data (don't run if this is the first pull)\n",
        "#end_time = pd.to_datetime(time.time()*1000000000)\n",
        "end_time = last_date + datetime.timedelta(days=7)\n",
        "Google_Trends_New_List = []\n",
        "for i in range(len(ISO_codes)): \n",
        "  print(country_labels[i])\n",
        "  for j in range(len(keyword_list)): \n",
        "    Google_Trends_Temp = downloading_google_trends(last_date.year, last_date.month, last_date.day, end_time.year, end_time.month, end_time.day, keyword_list[j], ISO_codes[i])\n",
        "    Google_Trends_Temp = Google_Trends_Temp.add_prefix(country_labels[i] + \"_\")\n",
        "    Google_Trends_Temp.columns.values[0] = \"Date\"\n",
        "    Google_Trends_Temp.drop_duplicates(keep='first', subset = \"Date\", inplace = True)\n",
        "    Google_Trends_New_List.append(Google_Trends_Temp)\n",
        "\n",
        "#Merge together\n",
        "Google_Trends_New = reduce(lambda x, y: pd.merge(x, y, on = 'Date'), Google_Trends_New_List)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "World\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "US\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "GB\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "Japan\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "India\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n",
            "The request failed: Google returned a response with code 500.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alc2uJPKpH93"
      },
      "source": [
        "###Append objects together (don't run if this is the first pull)\n",
        "#Normalise to the original object's ratio\n",
        "for i in range(1, Google_Trends.shape[1]):\n",
        "    try:\n",
        "      ratio = float(Google_Trends.iloc[-1,i])/float(Google_Trends_New.iloc[0,i]) \n",
        "    except ZeroDivisionError:\n",
        "      ratio = 1\n",
        "    Google_Trends_New.iloc[:,i] = Google_Trends_New.iloc[:,i]*ratio   \n",
        "\n",
        "#Append and delete duplicates\n",
        "Google_Trends = Google_Trends.append(Google_Trends_New).reset_index(drop=True)\n",
        "Google_Trends_New = pd.DataFrame()\n",
        "del(Google_Trends_New)\n",
        "Google_Trends.drop_duplicates(keep='first', subset = \"Date\", inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TqxxjZipMgH"
      },
      "source": [
        "#Save the data\n",
        "Google_Trends.to_csv(my_filepath + 'Raw Data/Google_Trends.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb0KbHIyxOoQ"
      },
      "source": [
        "## Chapter 4: Pull ledger data \n",
        "Pull data on the biggest accounts and their transactions from blockcypher. Again, this is designed for updating previously downloaded data, but can be adjusted for a first pull. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DueIhwfaJa5"
      },
      "source": [
        "# #Pull the 5000 accounts with the most bitcoin at the moment\n",
        "# Accounts_List = []\n",
        "# for i in range(50):\n",
        "#   response_temp = requests.get(\"https://api.blockchair.com/bitcoin/addresses\", params = {'limit': 100,\n",
        "#                                                                                   'offset': i*100})\n",
        "#   print(response_temp.status_code)\n",
        "#   response_temp = pd.DataFrame(json.loads(response_temp.text)['data'])\n",
        "#   Accounts_List.append(response_temp)\n",
        "#   time.sleep(3)\n",
        "# \n",
        "# Accounts = pd.concat(Accounts_List).reset_index(drop=True)\n",
        "# account = Accounts['address']\n",
        "# \n",
        "# #Save\n",
        "# account.to_csv(my_filepath + 'Raw Data/Accounts.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9wkgvQ2jCyE"
      },
      "source": [
        "#Read in list of the largest accounts\n",
        "accounts = pd.read_csv(my_filepath + 'Raw Data/Accounts.csv')['address']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNLB3Ijtz2g7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "3a9447c8-548c-424d-8388-ba4e9ca5162d"
      },
      "source": [
        "#Loop through, pull the data, and convert this to an hourly dataset with the balance for each account\n",
        "ledger_data_List = []\n",
        "dates = pd.DataFrame({'Date': pd.date_range(start=\"2001-01-01\",end=(datetime.datetime.today()+datetime.timedelta(days=1)).strftime('%Y-%m-%d'), freq = \"H\")})\n",
        "\n",
        "for j in range(100):\n",
        "  if j % 20 == 0:\n",
        "    print(j)\n",
        "  i = j + 500\n",
        "  #Pull the data, make an hour column, and clean (Try 3 times)\n",
        "  tries = 3\n",
        "  ledger_data_temp = pd.DataFrame(get_address_details(accounts[i], txn_limit = 50000)['txrefs'])[['ref_balance', 'confirmed']]\n",
        "  ledger_data_temp['confirmed'] = pd.to_datetime(ledger_data_temp['confirmed'], errors='coerce', utc=True).values.astype('datetime64[s]')\n",
        "  if ledger_data_temp[ledger_data_temp['confirmed'].isnull()].shape[0] > 0:\n",
        "    print(\"Account \" + str(i) + \" missing dates: \" + str(ledger_data_temp[ledger_data_temp['confirmed'].isnull()].shape[0]))\n",
        "  ledger_data_temp[ledger_data_temp['confirmed'].notnull()]\n",
        "  ledger_data_temp['Date'] = pd.Series(ledger_data_temp['confirmed']).dt.floor(\"H\")\n",
        "  ledger_data_temp.drop(labels = \"confirmed\", axis = 1, inplace = True)\n",
        "  ledger_data_temp['ref_balance'] = ledger_data_temp['ref_balance']/100000000\n",
        "\n",
        "  #Expand this to be a full hourly dataset and append to list\n",
        "  ledger_data_temp = pd.merge(dates, ledger_data_temp, how = 'left', on = 'Date')\n",
        "  ledger_data_temp.iloc[0,1] = 0\n",
        "  ledger_data_temp = ledger_data_temp.fillna(method='ffill')\n",
        "  ledger_data_temp = ledger_data_temp.groupby('Date')['ref_balance'].aggregate(balance = 'last')\n",
        "  ledger_data_temp.rename(columns = {'balance': \"balance_\" + str(i+1)}, inplace = True)\n",
        "  ledger_data_List.append(ledger_data_temp)\n",
        "  time.sleep(5)\n",
        "\n",
        "#Merge together the datasets\n",
        "ledger_data = reduce(lambda x, y: pd.merge(x, y, on = 'Date'), ledger_data_List)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-83a3b59dd262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#Pull the data, make an hour column, and clean (Try 3 times)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mledger_data_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_address_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxn_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'txrefs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ref_balance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confirmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mledger_data_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'confirmed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mledger_data_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'confirmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datetime64[s]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mledger_data_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mledger_data_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'confirmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/blockcypher/api.py\u001b[0m in \u001b[0;36mget_address_details\u001b[0;34m(address, coin_symbol, txn_limit, api_key, before_bh, after_bh, unspent_only, show_confidence, confirmations, include_script)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIMEOUT_IN_SECONDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clean_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/blockcypher/api.py\u001b[0m in \u001b[0;36mget_valid_json\u001b[0;34m(request, allow_204)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_valid_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_204\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Status Code 429'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m204\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallow_204\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: ('Status Code 429', '{\"error\": \"Limits reached.\"}\\n')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpwKosTRoI5N"
      },
      "source": [
        "#Open previous data\n",
        "ledger_data_old = pd.read_csv(filepath + 'Raw Data/Ledger_Data_All.csv').set_index('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAjnqW3yoEXH"
      },
      "source": [
        "#Merge\n",
        "ledger_data = pd.merge(ledger_data_old, ledger_data, left_index= True, right_index= True, how = 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsxcgVtWoHen"
      },
      "source": [
        "### Save data\n",
        "ledger_data.to_csv(my_filepath + 'Raw Data/Ledger_Data_All.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rswYD7n7b7z"
      },
      "source": [
        "### Add columns for summed balances\n",
        "#First count the number of non-zero columns per row\n",
        "cols = ledger_data.columns\n",
        "ledger_data_summary = pd.DataFrame({'non_zero': ledger_data[cols].gt(0).sum(axis=1)})\n",
        "#Then sum, and divide by the number of non-zero columns\n",
        "ledger_data_summary['adjusted_balance'] = ledger_data.sum(axis=1)/np.where(ledger_data_summary['non_zero']==0,1,ledger_data_summary['non_zero'])\n",
        "ledger_data_summary.drop(labels = \"non_zero\", axis = 1, inplace = True)\n",
        "#Finally, take only the bits from 2015 onwards\n",
        "ledger_data_summary = ledger_data_summary.iloc[120000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FZupk-t_Xad"
      },
      "source": [
        "### Save data\n",
        "ledger_data_summary.to_csv(my_filepath + 'Raw Data/Ledger_Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omASjEHJHKZL"
      },
      "source": [
        "## Chapter 5: Clean all datasets \n",
        "Where required, clean datasets (currently just Google Trends)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEB0GK0tHxDF"
      },
      "source": [
        "Clean trends data to account for missing data. Add the trend from first missing to last missing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzsBzDjzHl-O"
      },
      "source": [
        "###Open data\n",
        "Google_Trends = pd.read_csv(my_filepath + 'Raw Data/Google_Trends.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpDe_a4yN_G4"
      },
      "source": [
        "###Replace with missing if the world bitcoin data for that day is 0 (as this indicates missing, rather than actually 0)\n",
        "Google_Trends.loc[Google_Trends[\"World_Bitcoin\"] == 0, ['World_Ethereum','World_Coinbase',]] = np.nan, np.nan\n",
        "Google_Trends.loc[Google_Trends[\"World_Bitcoin\"] == 0, ['US_Bitcoin','US_Ethereum','US_Coinbase',]] = np.nan, np.nan, np.nan\n",
        "Google_Trends.loc[Google_Trends[\"World_Bitcoin\"] == 0, ['GB_Bitcoin','GB_Ethereum','GB_Coinbase',]] = np.nan, np.nan, np.nan\n",
        "Google_Trends.loc[Google_Trends[\"World_Bitcoin\"] == 0, ['Japan_Bitcoin','Japan_Ethereum','Japan_Coinbase',]] = np.nan, np.nan, np.nan\n",
        "Google_Trends.loc[Google_Trends[\"World_Bitcoin\"] == 0, ['India_Bitcoin','India_Ethereum','India_Coinbase',]] = np.nan, np.nan, np.nan\n",
        "Google_Trends.loc[Google_Trends[\"World_Bitcoin\"] == 0, 'World_Bitcoin'] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2FfbFhZPzeX"
      },
      "source": [
        "###Replace with the trend\n",
        "Google_Trends.interpolate(method='linear', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpPoHIgTRcIq"
      },
      "source": [
        "#Save\n",
        "Google_Trends.to_csv(my_filepath + 'Raw Data/Google_Trends_Filled.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC6womRPHKiP"
      },
      "source": [
        "## Chapter 6: Collapse data into period by period data\n",
        "Collapse into period by period statistics (i.e. statistics by hour rather than by transaction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xTvY74sxbR_"
      },
      "source": [
        "###Read in data and convert the datetime variable\n",
        "bitcoin_data = pd.read_csv(my_filepath + \"Raw Data/Raw_Data_Bitcoin.csv\")\n",
        "bitcoin_data['time'] = pd.to_datetime(bitcoin_data['time'])\n",
        "ethereum_data = pd.read_csv(my_filepath + \"Raw Data/Raw_Data_Ethereum.csv\")\n",
        "ethereum_data['time'] = pd.to_datetime(ethereum_data['time'])\n",
        "Google_Trends = pd.read_csv(my_filepath + 'Raw Data/Google_Trends_Filled.csv')\n",
        "Google_Trends['Date'] = pd.to_datetime(Google_Trends['Date'])\n",
        "ledger_data_summary = pd.read_csv(my_filepath + 'Raw Data/Ledger_Data.csv')\n",
        "ledger_data_summary['Date'] = pd.to_datetime(ledger_data_summary['Date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Umi9ROjRc6v"
      },
      "source": [
        "###Set time period for 1 hour \n",
        "time_period = \"1H\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK8HGTc1zLGV"
      },
      "source": [
        "###Create summary statistics by period for bitcoin\n",
        "cleaned_data_bitcoin = bitcoin_data.groupby(pd.Grouper(key = 'time', freq=time_period))['price'].aggregate(end_price_bitcoin = 'last')\n",
        "cleaned_data_bitcoin['mean_volume_bitcoin'] = bitcoin_data.groupby(pd.Grouper(key = 'time', freq=time_period))['volume'].aggregate(np.mean).to_frame()\n",
        "cleaned_data_bitcoin['number_sales_bitcoin'] = bitcoin_data[bitcoin_data['buy_sell']==\"s\"].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_bitcoin['number_sales_bitcoin'] = np.where(pd.isnull(cleaned_data_bitcoin['number_sales_bitcoin']),0,cleaned_data_bitcoin['number_sales_bitcoin'])\n",
        "cleaned_data_bitcoin['number_big_sales_bitcoin'] = bitcoin_data[(bitcoin_data['buy_sell']==\"s\") & ((bitcoin_data['price']*bitcoin_data['volume'])>10000)].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_bitcoin['number_big_sales_bitcoin'] = np.where(pd.isnull(cleaned_data_bitcoin['number_big_sales_bitcoin']),0,cleaned_data_bitcoin['number_big_sales_bitcoin'])\n",
        "cleaned_data_bitcoin['number_purchases_bitcoin'] = bitcoin_data[bitcoin_data['buy_sell']==\"b\"].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_bitcoin['number_purchases_bitcoin'] = np.where(pd.isnull(cleaned_data_bitcoin['number_purchases_bitcoin']),0,cleaned_data_bitcoin['number_purchases_bitcoin'])\n",
        "cleaned_data_bitcoin['number_big_purchases_bitcoin'] = bitcoin_data[(bitcoin_data['buy_sell']==\"b\") & ((bitcoin_data['price']*bitcoin_data['volume'])>10000)].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_bitcoin['number_big_purchases_bitcoin'] = np.where(pd.isnull(cleaned_data_bitcoin['number_purchases_bitcoin']),0,cleaned_data_bitcoin['number_purchases_bitcoin'])\n",
        "cleaned_data_bitcoin['number_market_bitcoin'] = bitcoin_data[bitcoin_data['market_limit']==\"m\"].groupby(pd.Grouper(key = 'time', freq=time_period))['market_limit'].aggregate('count').to_frame()\n",
        "cleaned_data_bitcoin['number_market_bitcoin'] = np.where(pd.isnull(cleaned_data_bitcoin['number_market_bitcoin']),0,cleaned_data_bitcoin['number_market_bitcoin'])\n",
        "cleaned_data_bitcoin['number_limit_bitcoin'] = bitcoin_data[bitcoin_data['market_limit']==\"l\"].groupby(pd.Grouper(key = 'time', freq=time_period))['market_limit'].aggregate('count').to_frame()\n",
        "cleaned_data_bitcoin['number_limit_bitcoin'] = np.where(pd.isnull(cleaned_data_bitcoin['number_limit_bitcoin']),0,cleaned_data_bitcoin['number_limit_bitcoin'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwlJgi1QF1Ny"
      },
      "source": [
        "###Create summary statistics by period for ethereum\n",
        "cleaned_data_ethereum = ethereum_data.groupby(pd.Grouper(key = 'time', freq=time_period))['price'].aggregate(end_price_ethereum = 'last')\n",
        "cleaned_data_ethereum['mean_volume_ethereum'] = ethereum_data.groupby(pd.Grouper(key = 'time', freq=time_period))['volume'].aggregate(np.mean).to_frame()\n",
        "cleaned_data_ethereum['number_sales_ethereum'] = ethereum_data[ethereum_data['buy_sell']==\"s\"].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_ethereum['number_sales_ethereum'] = np.where(pd.isnull(cleaned_data_ethereum['number_sales_ethereum']),0,cleaned_data_ethereum['number_sales_ethereum'])\n",
        "cleaned_data_ethereum['number_big_sales_ethereum'] = ethereum_data[(ethereum_data['buy_sell']==\"s\") & ((ethereum_data['price']*ethereum_data['volume'])>10000)].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_ethereum['number_big_sales_ethereum'] = np.where(pd.isnull(cleaned_data_ethereum['number_big_sales_ethereum']),0,cleaned_data_ethereum['number_big_sales_ethereum'])\n",
        "cleaned_data_ethereum['number_purchases_ethereum'] = ethereum_data[ethereum_data['buy_sell']==\"b\"].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_ethereum['number_purchases_ethereum'] = np.where(pd.isnull(cleaned_data_ethereum['number_purchases_ethereum']),0,cleaned_data_ethereum['number_purchases_ethereum'])\n",
        "cleaned_data_ethereum['number_big_purchases_ethereum'] = ethereum_data[(ethereum_data['buy_sell']==\"b\") & ((ethereum_data['price']*ethereum_data['volume'])>10000)].groupby(pd.Grouper(key = 'time', freq=time_period))['buy_sell'].aggregate('count').to_frame()\n",
        "cleaned_data_ethereum['number_big_purchases_ethereum'] = np.where(pd.isnull(cleaned_data_ethereum['number_big_purchases_ethereum']),0,cleaned_data_ethereum['number_big_purchases_ethereum'])\n",
        "cleaned_data_ethereum['number_market_ethereum'] = ethereum_data[ethereum_data['market_limit']==\"m\"].groupby(pd.Grouper(key = 'time', freq=time_period))['market_limit'].aggregate('count').to_frame()\n",
        "cleaned_data_ethereum['number_market_ethereum'] = np.where(pd.isnull(cleaned_data_ethereum['number_market_ethereum']),0,cleaned_data_ethereum['number_market_ethereum'])\n",
        "cleaned_data_ethereum['number_limit_ethereum'] = ethereum_data[ethereum_data['market_limit']==\"l\"].groupby(pd.Grouper(key = 'time', freq=time_period))['market_limit'].aggregate('count').to_frame()\n",
        "cleaned_data_ethereum['number_limit_ethereum'] = np.where(pd.isnull(cleaned_data_ethereum['number_limit_ethereum']),0,cleaned_data_ethereum['number_limit_ethereum'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow_tAeuHGX4m"
      },
      "source": [
        "###Merge data\n",
        "cleaned_data = pd.merge(cleaned_data_bitcoin, cleaned_data_ethereum, how = 'inner', on = 'time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKy-kPpZmN1w"
      },
      "source": [
        "#Delete unneeded objects\n",
        "del(cleaned_data_bitcoin, cleaned_data_ethereum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGFzVGbLoQVk"
      },
      "source": [
        "###Merge with the Google data\n",
        "cleaned_data = cleaned_data.reset_index()\n",
        "Google_Trends.rename(columns = {'Date':'time'}, inplace = True)\n",
        "cleaned_data = pd.merge(cleaned_data, Google_Trends, how = 'inner', on = 'time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp8ksdklBrUo"
      },
      "source": [
        "###Merge with the ledger data\n",
        "ledger_data_summary.rename(columns = {'Date':'time'}, inplace = True)\n",
        "cleaned_data = pd.merge(cleaned_data, ledger_data_summary, how = 'inner', on = 'time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om091-UoRyHs"
      },
      "source": [
        "###Save as cleaned data\n",
        "cleaned_data.to_csv(my_filepath + 'Cleaned Data/Data_Cleaned_1Hour.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhD4bFL0RTVQ"
      },
      "source": [
        "## Chapter 7: Lag the dataset\n",
        "Create a lagged dataset for feeding into the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG7_yQeOR2rm"
      },
      "source": [
        "###Read data\n",
        "cleaned_data = pd.read_csv(my_filepath + 'Cleaned Data/Data_Cleaned_1Hour.csv')\n",
        "cleaned_data['time'] = pd.to_datetime(cleaned_data['time'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt6Yn576bBpR"
      },
      "source": [
        "###Set the number of periods to be lagged\n",
        "number_periods = 500"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brRR1gvXdQQj",
        "outputId": "8560ffcf-cc88-4355-d7e9-fe3303115982"
      },
      "source": [
        "##Create lagged datasets for merge\n",
        "lagged_data = []\n",
        "#Loop through each of the lags and print progress occasionally\n",
        "for i in range(number_periods, -1, -1):\n",
        "  if (i % 10 == 0):\n",
        "      print(\"Lag:\", i)\n",
        "  #Create new dataset lagged by the amount\n",
        "  lagged_data_temp = cleaned_data.copy()\n",
        "  lagged_data_temp['time'] = lagged_data_temp['time'] + datetime.timedelta(hours=i)\n",
        "  lagged_data_temp.columns = lagged_data_temp.columns + \"_L\" + str(i)\n",
        "  lagged_data_temp.columns = lagged_data_temp.columns.str.replace('time_L'+str(i), 'time')\n",
        "  #Append to list\n",
        "  lagged_data.append(lagged_data_temp)\n",
        "\n",
        "#Merge all together\n",
        "lagged_data = reduce(lambda  left,right: pd.merge(left,right,on=['time'],\n",
        "                                            how='inner'), lagged_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lag: 500\n",
            "Lag: 490\n",
            "Lag: 480\n",
            "Lag: 470\n",
            "Lag: 460\n",
            "Lag: 450\n",
            "Lag: 440\n",
            "Lag: 430\n",
            "Lag: 420\n",
            "Lag: 410\n",
            "Lag: 400\n",
            "Lag: 390\n",
            "Lag: 380\n",
            "Lag: 370\n",
            "Lag: 360\n",
            "Lag: 350\n",
            "Lag: 340\n",
            "Lag: 330\n",
            "Lag: 320\n",
            "Lag: 310\n",
            "Lag: 300\n",
            "Lag: 290\n",
            "Lag: 280\n",
            "Lag: 270\n",
            "Lag: 260\n",
            "Lag: 250\n",
            "Lag: 240\n",
            "Lag: 230\n",
            "Lag: 220\n",
            "Lag: 210\n",
            "Lag: 200\n",
            "Lag: 190\n",
            "Lag: 180\n",
            "Lag: 170\n",
            "Lag: 160\n",
            "Lag: 150\n",
            "Lag: 140\n",
            "Lag: 130\n",
            "Lag: 120\n",
            "Lag: 110\n",
            "Lag: 100\n",
            "Lag: 90\n",
            "Lag: 80\n",
            "Lag: 70\n",
            "Lag: 60\n",
            "Lag: 50\n",
            "Lag: 40\n",
            "Lag: 30\n",
            "Lag: 20\n",
            "Lag: 10\n",
            "Lag: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-EOx6qZmBil"
      },
      "source": [
        "###Add hour of day and day or week\n",
        "lagged_data['hour_of_day'] = lagged_data['time'].dt.hour\n",
        "lagged_data['day_of_week'] = lagged_data['time'].dt.weekday"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fb034EkvOLc"
      },
      "source": [
        "#Set time as the index\n",
        "lagged_data.set_index('time', inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpLrgULMmTw2"
      },
      "source": [
        "###Save as cleaned data\n",
        "lagged_data.to_csv(my_filepath + 'Cleaned Data/Data_Cleaned_Lagged_1Hour.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXK4vBPXDkwp"
      },
      "source": [
        "##Chapter 8: Put into form for machine learning\n",
        "First, decide on the length of time to look at for the outcome data (i.e. predicting changes over the next hour, 3 hours etc.) If the length of time is longer than the length of each period in the data, then observations need to be dropped to avoid overlap between outcome data. To choose this, select the periods to be dropped (0 for the length of time equal to the length of each period, or higher for a longer length of time). \n",
        "Then split into train, test and validation as usual. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi43mkxD6drh"
      },
      "source": [
        "#Choose the length of time to look at for the outcome data (0 if equal to the length of each period, 1 is double, 2 triple etc.) \n",
        "periods_to_drop = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0i6TJPk7uDq"
      },
      "source": [
        "###Read in data and convert the datetime variable\n",
        "lagged_data_master = pd.read_csv(my_filepath + \"Cleaned Data/Data_Cleaned_Lagged_1Hour.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIf1B5v6GyAV"
      },
      "source": [
        "###Copy the dataset\n",
        "lagged_data = lagged_data_master.copy() #Do this to avoid having to reload every time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDXWzs7cFY1B"
      },
      "source": [
        "###Capture the number of time-varying variables\n",
        "number_vars = len(lagged_data.columns[pd.Series(lagged_data.columns).str.endswith((\"_L0\"))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnHgtA_WRFUA"
      },
      "source": [
        "###Create the outcome variable, which depends on the number of periods to drop\n",
        "lagged_data[\"Bitcoin_percentage_change\"] = 100*(lagged_data[\"end_price_bitcoin_L0\"]-lagged_data[\"end_price_bitcoin_L\"+str(periods_to_drop+1)])/lagged_data[\"end_price_bitcoin_L\"+str(periods_to_drop+1)]\n",
        "#Then drop all variables that we don't need (all L0 ones, plus any others if there are periods to drop)\n",
        "for i in range(0, periods_to_drop+1):\n",
        "  cols_to_drop = [col for col in lagged_data if col.endswith('L' + str(i))]\n",
        "  lagged_data.drop(axis = 1, columns = cols_to_drop, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni7e_lN6TC6E"
      },
      "source": [
        "#Drop rows to stop outcome measure overlap\n",
        "lagged_data = lagged_data[lagged_data.index % (periods_to_drop+1) == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdRxxWVwTDHg"
      },
      "source": [
        "###Change all the predictors to be percentage difference\n",
        "#Set up variables for exclusion (add \"_\" as well to capture some single and some double digit endings)\n",
        "for i in range(periods_to_drop+1, number_periods):\n",
        "  for j in [col for col in lagged_data if col.endswith('L' + str(i))]:\n",
        "    col_stub = j.replace(\"_L\" + str(i), \"\")\n",
        "    j_lag = col_stub + \"_L\" + str(i+1)\n",
        "    lagged_data[j] = (lagged_data[j] - lagged_data[j_lag])/np.where(lagged_data[j] == 0, 0.1, lagged_data[j])\n",
        "\n",
        "#Remove the final lag\n",
        "cols_to_drop = [col for col in lagged_data if col.endswith('L' + str(number_periods))]\n",
        "lagged_data.drop(axis = 1, columns = cols_to_drop, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNd-gusuTDOE"
      },
      "source": [
        "###Drop any with NA or inf\n",
        "lagged_data = lagged_data.replace(-np.inf, np.nan)\n",
        "lagged_data = lagged_data.replace(np.inf, np.nan)\n",
        "lagged_data = lagged_data.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb_2XQW-EyCh"
      },
      "source": [
        "###Split into train, val and test\n",
        "#First split out the test set. This should be the most recent observations\n",
        "full_data_train_val, full_data_test  = np.split(lagged_data, [int(0.9*len(lagged_data))])\n",
        "\n",
        "#Then split the remainder into train and validation\n",
        "np.random.seed(seed_object+1)\n",
        "rand = np.random.rand(len(full_data_train_val))\n",
        "msk_train = rand < 0.8\n",
        "full_data_train = full_data_train_val.iloc[msk_train]\n",
        "full_data_val = full_data_train_val.iloc[~msk_train]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idCHwdLXG4oT"
      },
      "source": [
        "###Make train data balanced\n",
        "#Create a rounded outcome variable and check it's balance\n",
        "full_data_train['binary'] = np.where(full_data_train[\"Bitcoin_percentage_change\"]>0, 1, 0)\n",
        "full_data_train['binary'].value_counts()\n",
        "#Drop randomly from the overweighted group\n",
        "full_data_train_1 = full_data_train[full_data_train['binary'] == 1]\n",
        "full_data_train_0 = full_data_train[full_data_train['binary'] == 0]\n",
        "np.random.seed(seed_object)\n",
        "full_data_train_1 = full_data_train_1.sample(full_data_train['binary'].value_counts()[0])\n",
        "full_data_train = full_data_train_0.append(full_data_train_1)\n",
        "#Check that the oucome is balanced\n",
        "print(full_data_train['binary'].value_counts())\n",
        "full_data_train.drop(axis = 1, columns = 'binary', inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omgE9zvL5EhV"
      },
      "source": [
        "#Make outcome data binary\n",
        "full_data_train['Bitcoin_percentage_change'] = np.where(full_data_train['Bitcoin_percentage_change']>0, 1, 0)\n",
        "full_data_val['Bitcoin_percentage_change'] = np.where(full_data_val['Bitcoin_percentage_change']>0, 1, 0)\n",
        "full_data_test['Bitcoin_percentage_change'] = np.where(full_data_test['Bitcoin_percentage_change']>0, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UNgbQZLs1_c"
      },
      "source": [
        "#Split out data into X and Y\n",
        "y_train = full_data_train[[\"Bitcoin_percentage_change\"]]\n",
        "x_train = full_data_train.drop(axis = 1, columns =  \"Bitcoin_percentage_change\")\n",
        "y_val = full_data_val[[\"Bitcoin_percentage_change\"]]\n",
        "x_val = full_data_val.drop(axis = 1, columns = \"Bitcoin_percentage_change\")\n",
        "y_test = full_data_test[[\"Bitcoin_percentage_change\"]]\n",
        "x_test = full_data_test.drop(axis = 1, columns =  \"Bitcoin_percentage_change\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpQGT_n11XaV"
      },
      "source": [
        "#Scale the data\n",
        "scaler = StandardScaler()\n",
        "x_train = pd.DataFrame(scaler.fit_transform(x_train), columns = x_train.columns, index = x_train.index)\n",
        "x_val = pd.DataFrame(scaler.transform(x_val), columns = x_val.columns, index = x_val.index)\n",
        "x_test = pd.DataFrame(scaler.transform(x_test), columns = x_test.columns, index = x_test.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJq3b38CsD7X"
      },
      "source": [
        "##Chapter 9: Run basic machine learning\n",
        "Try first with an Elasticnet and Random forest to give a quick check and to look for important variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScgkA2cnsCom"
      },
      "source": [
        "#Set up gridsearch model for elasticnet\n",
        "gsc = GridSearchCV(\n",
        "        estimator=LogisticRegression(penalty = 'elasticnet', solver='saga',\n",
        "                                     random_state = 71166, max_iter = 150),\n",
        "        param_grid={\n",
        "            'C': [0.000005, 0.00005, 0.0005],\n",
        "            'l1_ratio': [0, 0.3, 0.7, 1]\n",
        "        },\n",
        "        scoring = {'Accuracy': 'accuracy', 'Balanced_Accuracy': 'balanced_accuracy', \n",
        "                   'F1':'f1', 'AUC': 'roc_auc'}, \n",
        "        refit='AUC',\n",
        "        cv=2, verbose=10, n_jobs = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY8X4NB_sq-G"
      },
      "source": [
        "#Fit elasticnet model\n",
        "model = gsc.fit(x_train, y_train.values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9gx0fcvstGZ"
      },
      "source": [
        "#Show elasticnet results\n",
        "pd.DataFrame(model.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unF3nKmsuyGX"
      },
      "source": [
        "#Get the most important variables for elasticnet\n",
        "importance = pd.DataFrame({\"Feature\": x_train.columns,\n",
        "                           \"stdev\": np.std(x_train, 0),\n",
        "                           \"coef\": model.best_estimator_.coef_[0]})\n",
        "importance['Importance'] = 1000*importance['stdev']*importance['coef']\n",
        "importance.sort_values(by = 'Importance', ascending = False, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_HamXH3uy2S"
      },
      "source": [
        "#Show the most important\n",
        "importance.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yTnBAPa6rCa"
      },
      "source": [
        "#Set up gridsearch model for random forest\n",
        "gsc = GridSearchCV(\n",
        "        estimator=RandomForestClassifier(verbose = 10),\n",
        "        param_grid={\n",
        "            'n_estimators': [100, 150],\n",
        "            'min_samples_split': [50, 200]\n",
        "        },\n",
        "        scoring = {'Accuracy': 'accuracy', 'Balanced_Accuracy': 'balanced_accuracy', \n",
        "                   'F1':'f1', 'AUC': 'roc_auc'}, \n",
        "        refit='AUC',\n",
        "        cv=2, verbose=10, n_jobs = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjxXof4c6rKw"
      },
      "source": [
        "#Fit model for random forest\n",
        "model = gsc.fit(x_train, y_train.values.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwfqfJIz6rNn"
      },
      "source": [
        "#Show results for random forest\n",
        "pd.DataFrame(model.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH5A9z7YsJwG"
      },
      "source": [
        "##Chapter 10: Put into form for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AebhvUXGwyo"
      },
      "source": [
        "#Split X vars into time varying and time invariant components and put into numpy arrays\n",
        "y_train = y_train.values\n",
        "x_train_static = x_train[['hour_of_day', 'day_of_week']].values\n",
        "x_train_time = x_train.drop(axis = 1, columns =  ['hour_of_day', 'day_of_week']).values\n",
        "y_val = y_val.values\n",
        "x_val_static = x_val[['hour_of_day', 'day_of_week']].values\n",
        "x_val_time = x_val.drop(axis = 1, columns = ['hour_of_day', 'day_of_week']).values\n",
        "y_test = y_test.values\n",
        "x_test_static = x_test[['hour_of_day', 'day_of_week']].values\n",
        "x_test_time = x_test.drop(axis = 1, columns =  ['hour_of_day', 'day_of_week']).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wCDLfkLhDcn"
      },
      "source": [
        "#Reshape time variant data to (observation,month,variables)\n",
        "x_train_time = x_train_time.reshape((full_data_train.shape[0],number_periods-periods_to_drop-1,number_vars))\n",
        "x_val_time = x_val_time.reshape((full_data_val.shape[0],number_periods-periods_to_drop-1,number_vars))\n",
        "x_test_time = x_test_time.reshape((full_data_test.shape[0],number_periods-periods_to_drop-1,number_vars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NzXY08TSKhx"
      },
      "source": [
        "##Chapter 11: Create and fit a LSTM model\n",
        "Run a model which uses LSTM layers for the time varying data and fully connected layers for the time invariant data, and which joins them at the top. Explore how well this works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZjOnzNOmBS"
      },
      "source": [
        "##Model\n",
        "#First the LSTM side\n",
        "input_tensor_1 = Input(shape=(number_periods-periods_to_drop-1, number_vars))\n",
        "X_1 = LSTM(units = 16, return_sequences = True, kernel_initializer=keras.initializers.glorot_uniform(seed=seed_object))(input_tensor_1)\n",
        "X_1 = LSTM(units = 8, return_sequences = False, kernel_initializer=keras.initializers.glorot_uniform(seed=seed_object))(X_1)\n",
        "\n",
        "#Fully connected\n",
        "input_tensor_2 = Input(shape = (2,)) \n",
        "X_2 = Dense(16, activation=\"relu\")(input_tensor_2)\n",
        "X_2 = Dropout(0.1)(X_2)\n",
        "\n",
        "#Join together\n",
        "X_3 = keras.layers.concatenate([X_1,X_2])\n",
        "X_3 = Dense(128, activation=\"relu\")(X_3)\n",
        "X_3 = Dropout(0.05)(X_3)\n",
        "X_3 = Dense(64, activation=\"relu\")(X_3)\n",
        "X_3 = Dropout(0.05)(X_3)\n",
        "out = Dense(1, activation=\"sigmoid\")(X_3)\n",
        "model = Model([input_tensor_1, input_tensor_2], out) \n",
        "\n",
        "#Compile the model and create callbacks\n",
        "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.9999, decay=0.00001)\n",
        "#model.compile(loss='mean_squared_error', optimizer=opt, metrics=[\"mae\"])\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "filepath = my_filepath + \"Models/model_1h.h5\"\n",
        "#checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=False, \n",
        "#                             save_best_only=True, mode='min')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=False, \n",
        "                             save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#Fit the model\n",
        "history = model.fit([x_train_time, x_train_static], y_train, batch_size = 512, epochs=20,\n",
        "          validation_data = ([x_val_time, x_val_static], y_val), shuffle=True,\n",
        "          verbose = True, callbacks=callbacks_list) #, class_weight = weights_train) #, sample_weight = weights_train)\n",
        "\n",
        "#Print the accuracy and correlation\n",
        "model_best = load_model(my_filepath + 'Models/model_1h.h5')\n",
        "full_data_train['prediction'] = model_best.predict([x_train_time, x_train_static])\n",
        "#full_data_train['prediction_rounded'] = np.where(full_data_train['prediction'] > 0,1,0)\n",
        "full_data_train['prediction_rounded'] = np.where(full_data_train['prediction'] > 0.5,1,0)\n",
        "full_data_train['actual_rounded'] = np.where(full_data_train[\"Bitcoin_percentage_change\"] > 0,1,0)\n",
        "correlation_temp_train, pvalue_temp_train = stats.pearsonr(full_data_train['prediction'], full_data_train['Bitcoin_percentage_change'])\n",
        "full_data_val['prediction'] = model_best.predict([x_val_time, x_val_static])\n",
        "#full_data_val['prediction_rounded'] = np.where(full_data_val['prediction'] > 0,1,0)\n",
        "full_data_val['prediction_rounded'] = np.where(full_data_val['prediction'] > 0.5,1,0)\n",
        "full_data_val['actual_rounded'] = np.where(full_data_val[\"Bitcoin_percentage_change\"] > 0,1,0)\n",
        "correlation_temp_val, pvalue_temp_val = stats.pearsonr(full_data_val['prediction'], full_data_val['Bitcoin_percentage_change'])\n",
        "print(\"Accuracy train: \" + str(sum(full_data_train['prediction_rounded']==full_data_train['actual_rounded'])/full_data_train.shape[0]))\n",
        "print(\"Correlation train: \" + str(correlation_temp_train))\n",
        "print(\"Accuracy val: \" + str(sum(full_data_val['prediction_rounded']==full_data_val['actual_rounded'])/full_data_val.shape[0]))\n",
        "print(\"Correlation val: \" + str(correlation_temp_val))\n",
        "\n",
        "# Delete objects\n",
        "del(input_tensor_1, X_1, out, filepath, checkpoint, callbacks_list, correlation_temp_train, pvalue_temp_train, correlation_temp_val, pvalue_temp_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayD2xz0yPgbs"
      },
      "source": [
        "#Explore the balance of predictions\n",
        "full_data_train['prediction_rounded'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpuiS52JQpl5"
      },
      "source": [
        "##Chapter 12: Explore results on the test set\n",
        "Run several times and take aggregate results on the test dataframe. Need to do this as we can't set seed when using a GPU, so want to take aggregated results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UggCoYtPPCfU"
      },
      "source": [
        "#Set key variables\n",
        "epochs = 20\n",
        "number_runs = 20\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5EFsmt_Yelb"
      },
      "source": [
        "#Try the model several times to get the outcome metrics\n",
        "results_models_list = []\n",
        "\n",
        "for j in range(number_runs):\n",
        "  print(j)\n",
        "  ##Model\n",
        "  #First the LSTM side\n",
        "  input_tensor_1 = Input(shape=(number_periods-periods_to_drop-1, number_vars))\n",
        "  X_1 = LSTM(units = 16, return_sequences = True, kernel_initializer=keras.initializers.glorot_uniform(seed=seed_object))(input_tensor_1)\n",
        "  X_1 = LSTM(units = 8, return_sequences = False, kernel_initializer=keras.initializers.glorot_uniform(seed=seed_object))(X_1)\n",
        "\n",
        "  #Fully connected\n",
        "  input_tensor_2 = Input(shape = (2,)) \n",
        "  X_2 = Dense(16, activation=\"relu\")(input_tensor_2)\n",
        "  X_2 = Dropout(0.1)(X_2)\n",
        "\n",
        "  #Join together\n",
        "  X_3 = keras.layers.concatenate([X_1,X_2])\n",
        "  X_3 = Dense(128, activation=\"relu\")(X_3)\n",
        "  X_3 = Dropout(0.05)(X_3)\n",
        "  X_3 = Dense(64, activation=\"relu\")(X_3)\n",
        "  X_3 = Dropout(0.05)(X_3)\n",
        "  out = Dense(1, activation=\"sigmoid\")(X_3)\n",
        "  model = Model([input_tensor_1, input_tensor_2], out) \n",
        "\n",
        "  #Compile the model and create callbacks\n",
        "  opt = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.9999, decay=0.00001)\n",
        "  #model.compile(loss='mean_squared_error', optimizer=opt, metrics=[\"mae\"])\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
        "  filepath = my_filepath + \"Models/model_1h.h5\"\n",
        "  #checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=False, \n",
        "  #                             save_best_only=True, mode='min')\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=False, \n",
        "                               save_best_only=True, mode='max')\n",
        "  callbacks_list = [checkpoint]\n",
        "\n",
        "  #Fit the model\n",
        "  history = model.fit([x_train_time, x_train_static], y_train, batch_size = 512, epochs=epochs,\n",
        "          validation_data = ([x_val_time, x_val_static], y_val), shuffle=True,\n",
        "            verbose = True, callbacks=callbacks_list) #, class_weight = weights_train) #, sample_weight = weights_train)\n",
        "\n",
        "  # Delete objects\n",
        "  del(input_tensor_1, X_1, out, filepath, checkpoint, callbacks_list)\n",
        "\n",
        "  #Read models in\n",
        "  model_best = load_model(my_filepath + 'Models/model_1h.h5')\n",
        "\n",
        "  #Evaluate model\n",
        "  train_loss, train_mae = model_best.evaluate([x_train_time, x_train_static], y_train, verbose = False)\n",
        "  val_loss, val_mae = model_best.evaluate([x_val_time, x_val_static], y_val, verbose = False)\n",
        "  test_loss, test_mae = model_best.evaluate([x_test_time, x_test_static], y_test, verbose = False)\n",
        "\n",
        "  #Predict outcomes for the test, train and validation set\n",
        "  full_data_test['prediction'] = model_best.predict([x_test_time, x_test_static])\n",
        "  #full_data_test['prediction_rounded'] = np.where(full_data_test['prediction'] > 0,1,0)\n",
        "  full_data_test['prediction_rounded'] = np.where(full_data_test['prediction'] > 0.5,1,0)\n",
        "  full_data_test['actual_rounded'] = np.where(full_data_test[\"Bitcoin_percentage_change\"] > 0,1,0)\n",
        "  #full_data_test['prediction_rounded_2'] = np.where(full_data_test['prediction'] >= 0.01,2,np.where(full_data_test['prediction'] <= -0.01,0,1))\n",
        "  full_data_test['prediction_rounded_2'] = np.where(full_data_test['prediction'] >= 0.51,2,np.where(full_data_test['prediction'] <= 0.049,0,1))\n",
        "  full_data_train['prediction'] = model_best.predict([x_train_time, x_train_static])\n",
        "  #full_data_train['prediction_rounded'] = np.where(full_data_train['prediction'] > 0,1,0)\n",
        "  full_data_train['prediction_rounded'] = np.where(full_data_train['prediction'] > 0.5,1,0)\n",
        "  full_data_train['actual_rounded'] = np.where(full_data_train[\"Bitcoin_percentage_change\"] > 0,1,0)\n",
        "  full_data_val['prediction'] = model_best.predict([x_val_time, x_val_static])\n",
        "  #full_data_val['prediction_rounded'] = np.where(full_data_val['prediction'] > 0,1,0)\n",
        "  full_data_val['prediction_rounded'] = np.where(full_data_val['prediction'] > 0.5,1,0)\n",
        "  full_data_val['actual_rounded'] = np.where(full_data_val[\"Bitcoin_percentage_change\"] > 0,1,0)\n",
        "\n",
        "  #Calculate the accuracy for each\n",
        "  Accuracy_temp_train = sum(full_data_train['prediction_rounded']==full_data_train['actual_rounded'])/full_data_train.shape[0]\n",
        "  Accuracy_temp_val = sum(full_data_val['prediction_rounded']==full_data_val['actual_rounded'])/full_data_val.shape[0]\n",
        "  Accuracy_temp_test = sum(full_data_test['prediction_rounded']==full_data_test['actual_rounded'])/full_data_test.shape[0]\n",
        "  try:\n",
        "    Accuracy_temp_test_m05 = sum(full_data_test[full_data_test['Bitcoin_percentage_change'] < -0.5]['prediction_rounded']==full_data_test[full_data_test['Bitcoin_percentage_change'] < -0.5]['actual_rounded'])/full_data_test[full_data_test['Bitcoin_percentage_change'] < -0.5].shape[0]\n",
        "  except ZeroDivisionError: \n",
        "    Accuracy_temp_test_m05 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_m025 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.25) & (full_data_test['Bitcoin_percentage_change'] > -0.5)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.25) & (full_data_test['Bitcoin_percentage_change'] > -0.5)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.25) & (full_data_test['Bitcoin_percentage_change'] > -0.5)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_m025 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_m01 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.1) & (full_data_test['Bitcoin_percentage_change'] > -0.25)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.1) & (full_data_test['Bitcoin_percentage_change'] > -0.25)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.1) & (full_data_test['Bitcoin_percentage_change'] > -0.25)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_m01 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_m005 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.05) & (full_data_test['Bitcoin_percentage_change'] > -0.1)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.05) & (full_data_test['Bitcoin_percentage_change'] > -0.1)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] < -0.05) & (full_data_test['Bitcoin_percentage_change'] > -0.1)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_m005 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_m0 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] < 0) & (full_data_test['Bitcoin_percentage_change'] > -0.05)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] < 0) & (full_data_test['Bitcoin_percentage_change'] > -0.05)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] < 0) & (full_data_test['Bitcoin_percentage_change'] > -0.05)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_m0 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_0 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] >= 0) & (full_data_test['Bitcoin_percentage_change'] < 0.05)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] >= 0) & (full_data_test['Bitcoin_percentage_change'] < 0.05)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] >= 0) & (full_data_test['Bitcoin_percentage_change'] < 0.05)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_0 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_005 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.05) & (full_data_test['Bitcoin_percentage_change'] < 0.1)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.05) & (full_data_test['Bitcoin_percentage_change'] < 0.1)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.05) & (full_data_test['Bitcoin_percentage_change'] < 0.1)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_005 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_01 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.1) & (full_data_test['Bitcoin_percentage_change'] < 0.25)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.1) & (full_data_test['Bitcoin_percentage_change'] < 0.25)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.1) & (full_data_test['Bitcoin_percentage_change'] < 0.25)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_01 = 0\n",
        "  try:\n",
        "    Accuracy_temp_test_025 = sum(full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.25) & (full_data_test['Bitcoin_percentage_change'] < 0.5)]['prediction_rounded']==full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.25) & (full_data_test['Bitcoin_percentage_change'] < 0.5)]['actual_rounded'])/full_data_test[(full_data_test['Bitcoin_percentage_change'] > 0.25) & (full_data_test['Bitcoin_percentage_change'] < 0.5)].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_025 = Accuracy_temp_test_025\n",
        "  try:\n",
        "    Accuracy_temp_test_05 = sum(full_data_test[full_data_test['Bitcoin_percentage_change'] > 0.5]['prediction_rounded']==full_data_test[full_data_test['Bitcoin_percentage_change'] > 0.5]['actual_rounded'])/full_data_test[full_data_test['Bitcoin_percentage_change'] > 0.5].shape[0]\n",
        "  except ZeroDivisionError:\n",
        "    Accuracy_temp_test_05 = 0\n",
        "  correlation_temp, pvalue_temp = stats.pearsonr(full_data_test['prediction'], full_data_test['Bitcoin_percentage_change'])\n",
        "\n",
        "  ### Calculate what would happen to $100 invested at the start (no transaction fee) with basic strategy\n",
        "  #Set start date (bottom) with 100\n",
        "  full_data_test['invested_basic'] = 0\n",
        "  full_data_test['invested_basic'].iloc[-1] = 100\n",
        "  #Loop upwards, calculating the next row each time\n",
        "  for i in range(full_data_test.shape[0]-1):\n",
        "    #Calculate the new value\n",
        "    row_number_temp = full_data_test.shape[0]-2-i\n",
        "    full_data_test['invested_basic'].iloc[row_number_temp] = full_data_test['invested_basic'].iloc[row_number_temp+1] * np.where(full_data_test['prediction_rounded'].iloc[row_number_temp] == 1, 1 + full_data_test['Bitcoin_percentage_change'].iloc[row_number_temp]/100, 1)\n",
        "\n",
        "  ### Calculate what would happen to $100 invested at the start (with a transaction fee) with basic strategy\n",
        "  #Set start date (bottom) with 100\n",
        "  full_data_test['invested_fee_basic'] = 0\n",
        "  full_data_test['invested_fee_basic'].iloc[-1] = 100\n",
        "  #Loop upwards, calculating the next row each time\n",
        "  for i in range(full_data_test.shape[0]-1):\n",
        "    #Calculate the new value\n",
        "    row_number_temp = full_data_test.shape[0]-2-i\n",
        "    full_data_test['invested_fee_basic'].iloc[row_number_temp] = full_data_test['invested_fee_basic'].iloc[row_number_temp+1] * np.where(full_data_test['prediction_rounded'].iloc[row_number_temp] == 1, 1 + full_data_test['Bitcoin_percentage_change'].iloc[row_number_temp]/100, 1)\n",
        "    #Minus the fee if there was a change\n",
        "    full_data_test['invested_fee_basic'].iloc[row_number_temp] = full_data_test['invested_fee_basic'].iloc[row_number_temp] - np.where((full_data_test['prediction_rounded'].iloc[row_number_temp] == 1) & (full_data_test['prediction_rounded'].iloc[row_number_temp+1] == 0), full_data_test['invested_fee_basic'].iloc[row_number_temp+1] * 0.0021, 0)\n",
        "    full_data_test['invested_fee_basic'].iloc[row_number_temp] = full_data_test['invested_fee_basic'].iloc[row_number_temp] - np.where((full_data_test['prediction_rounded'].iloc[row_number_temp] == 0) & (full_data_test['prediction_rounded'].iloc[row_number_temp+1] == 1), full_data_test['invested_fee_basic'].iloc[row_number_temp+1] * 0.0021, 0)\n",
        "\n",
        "  ### Calculate what would happen to $100 invested at the start (with a transaction fee) with complex strategy\n",
        "  #Set start date (bottom) with 100\n",
        "  full_data_test['invested_fee_complex'] = 0\n",
        "  full_data_test['invested_fee_complex'].iloc[-1] = 100\n",
        "  #Loop upwards, calculating the next row each time\n",
        "  for i in range(full_data_test.shape[0]-1):\n",
        "    row_number_temp = full_data_test.shape[0]-2-i\n",
        "    #Calculate the new value\n",
        "    if i == 0:\n",
        "      #Start conservatively. Only if the prediction is buy do you sell. Otherwise keep (and change prediction to 0)\n",
        "      full_data_test['invested_fee_complex'].iloc[row_number_temp] = full_data_test['invested_fee_complex'].iloc[row_number_temp+1] * np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] == 2, 1 + full_data_test['Bitcoin_percentage_change'].iloc[row_number_temp]/100, 1)\n",
        "      full_data_test['prediction_rounded_2'].iloc[row_number_temp]  = np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] in [0,1], 0, 2)\n",
        "    else:\n",
        "      #For holding previously\n",
        "      if full_data_test['prediction_rounded_2'].iloc[row_number_temp+1] == 2:\n",
        "        #If 1 or 2, then keep (and change prediction to 2). If 0, then sell (and keep prediction as 0)\n",
        "        full_data_test['invested_fee_complex'].iloc[row_number_temp] = full_data_test['invested_fee_complex'].iloc[row_number_temp+1] * np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] in [1,2], 1 + full_data_test['Bitcoin_percentage_change'].iloc[row_number_temp]/100, 1)\n",
        "        full_data_test['prediction_rounded_2'].iloc[row_number_temp]  = np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] in [1,2], 2, 0)\n",
        "        #If 0, then minus fees\n",
        "        full_data_test['invested_fee_complex'].iloc[row_number_temp] = full_data_test['invested_fee_complex'].iloc[row_number_temp] - np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] == 0, full_data_test['invested_fee_complex'].iloc[row_number_temp] * 0.0021, 0)\n",
        "      #For not holding previously\n",
        "      if full_data_test['prediction_rounded_2'].iloc[row_number_temp+1] == 0:\n",
        "        #If 0 or 1, then don't buy (and change prediction to 0). If 2, then buy (and keep prediction as 2)\n",
        "        full_data_test['invested_fee_complex'].iloc[row_number_temp] = full_data_test['invested_fee_complex'].iloc[row_number_temp+1] * np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] == 2, 1 + full_data_test['Bitcoin_percentage_change'].iloc[row_number_temp]/100, 1)\n",
        "        full_data_test['prediction_rounded_2'].iloc[row_number_temp]  = np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] in [0,1], 0, 2)\n",
        "        #If 2, then minus fees\n",
        "        full_data_test['invested_fee_complex'].iloc[row_number_temp] = full_data_test['invested_fee_complex'].iloc[row_number_temp] - np.where(full_data_test['prediction_rounded_2'].iloc[row_number_temp] == 2, full_data_test['invested_fee_complex'].iloc[row_number_temp] * 0.0021, 0)\n",
        "\n",
        "  ### Compare to simply leaving the investment as is\n",
        "  #Set start date (bottom) with 100\n",
        "  full_data_test['invested_simple'] = 0\n",
        "  full_data_test['invested_simple'].iloc[-1] = 100\n",
        "  #Loop upwards, calculating the next row each time\n",
        "  for i in range(full_data_test.shape[0]-1):\n",
        "    #Calculate the new value\n",
        "    row_number_temp = full_data_test.shape[0]-2-i\n",
        "    full_data_test['invested_simple'].iloc[row_number_temp] = full_data_test['invested_simple'].iloc[row_number_temp+1] * (1 + (full_data_test['Bitcoin_percentage_change'].iloc[row_number_temp])/100)\n",
        "\n",
        "  #Calculate final values\n",
        "  simple_temp = full_data_test['invested_simple'].iloc[0]\n",
        "  basic_temp = full_data_test['invested_basic'].iloc[0]\n",
        "  basic_fee_temp = full_data_test['invested_fee_basic'].iloc[0]\n",
        "  complex_fee_temp = full_data_test['invested_fee_complex'].iloc[0]\n",
        "\n",
        "  #Add items to list of results\n",
        "  results_models_list.append([correlation_temp, pvalue_temp, Accuracy_temp_train, Accuracy_temp_val, Accuracy_temp_test, Accuracy_temp_test_m05, Accuracy_temp_test_m025, Accuracy_temp_test_m01, Accuracy_temp_test_m005, Accuracy_temp_test_m0, Accuracy_temp_test_0, Accuracy_temp_test_005, Accuracy_temp_test_01, Accuracy_temp_test_025, Accuracy_temp_test_05, simple_temp, basic_temp, basic_fee_temp, complex_fee_temp])\n",
        "\n",
        "results_models = pd.DataFrame(results_models_list, columns=[\"Correlation\", \"P-value\", \"Accuracy_train\", \"Accuracy_val\", \"Accuracy_test\", \"Accuracy_test_m05\", \"Accuracy_test_m025\", \"Accuracy_test_m01\", \"Accuracy_test_m005\", \"Accuracy_test_m0\", \"Accuracy_test_0\", \"Accuracy_test_005\", \"Accuracy_test_01\", \"Accuracy_test_025\", \"Accuracy_test_05\",\"Simple\", \"Basic_No_Fee\", \"Basic_Fee\", \"Complex_Fee\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySIgXc1Ki0Xd"
      },
      "source": [
        "#Print aggregated results\n",
        "print(\"Train accuracy: \" + str(results_models['Accuracy_train'].agg('mean')))\n",
        "print(\"Validation accuracy: \" + str(results_models['Accuracy_val'].agg('mean')))\n",
        "print(\"Test accuracy: \" + str(results_models['Accuracy_test'].agg('mean')))\n",
        "print(\"Correlation coefficient: \" + str(results_models['Correlation'].agg('mean')))\n",
        "print(\"Correlation coefficient > 0: \" + str(results_models[results_models['Correlation'] >=0].shape[0]/results_models.shape[0]))\n",
        "print(\"Correlation p-value: \" + str(results_models['P-value'].agg('mean')))\n",
        "print(\"Accuracy test < -0.05: \" + str(results_models['Accuracy_test_m05'].agg('mean')))\n",
        "print(\"Accuracy test -0.05 - -0.025: \" + str(results_models['Accuracy_test_m025'].agg('mean')))\n",
        "print(\"Accuracy test -0.025 - -0.01: \" + str(results_models['Accuracy_test_m01'].agg('mean')))\n",
        "print(\"Accuracy test -0.01 - -0.005: \" + str(results_models['Accuracy_test_m005'].agg('mean')))\n",
        "print(\"Accuracy test -0.005-0: \" + str(results_models['Accuracy_test_m0'].agg('mean')))\n",
        "print(\"Accuracy test 0-0.005: \" + str(results_models['Accuracy_test_0'].agg('mean')))\n",
        "print(\"Accuracy test 0.005 - 0.01: \" + str(results_models['Accuracy_test_005'].agg('mean')))\n",
        "print(\"Accuracy test 0.01 - 0.025: \" + str(results_models['Accuracy_test_01'].agg('mean')))\n",
        "print(\"Accuracy test 0.025 - 0.05: \" + str(results_models['Accuracy_test_025'].agg('mean')))\n",
        "print(\"Accuracy test > 0.05: \" + str(results_models['Accuracy_test_05'].agg('mean')))\n",
        "print(\"Simple end: \" + str(results_models['Simple'].agg('mean')))\n",
        "print(\"Basic no fee end: \" + str(results_models['Basic_No_Fee'].agg('mean')))\n",
        "print(\"Basic fee end: \" + str(results_models['Basic_Fee'].agg('mean')))\n",
        "print(\"Complex fee end: \" + str(results_models['Complex_Fee'].agg('mean')))\n",
        "print(\"Basic no fee beats simple: \" + str(100*sum(results_models['Basic_No_Fee']>results_models['Simple'])/results_models.shape[0]))\n",
        "print(\"Basic fee beats simple: \" + str(100*sum(results_models['Basic_Fee']>results_models['Simple'])/results_models.shape[0]))\n",
        "print(\"Complex fee beats simple: \" + str(100*sum(results_models['Complex_Fee']>results_models['Simple'])/results_models.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdJELzt7l7Qe"
      },
      "source": [
        "#Save all results for exploring\n",
        "results_models.to_csv(my_filepath + 'Results/Repeat_Model_Results_higher.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPTqIqJcmLIK"
      },
      "source": [
        "#Save the full data, but with only the last couple of months\n",
        "full_data_test.iloc[:,number_vars*(number_periods-periods_to_drop-3):].to_csv(my_filepath + 'Results/Test_Date.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwTzozw5oUL"
      },
      "source": [
        "##Chapter 13: Try copying accounts\n",
        "Explore what happens if you copy some of the more active whales. Work in progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrCGvN8a54Fy"
      },
      "source": [
        "###Read in data\n",
        "cleaned_data = pd.read_csv(my_filepath + 'Cleaned Data/Data_Cleaned_1Hour.csv', index_col = 'time')\n",
        "ledger_data = pd.read_csv(my_filepath + 'Raw Data/Ledger_Data_All.csv', index_col = 'Date')\n",
        "account_names = pd.read_csv(my_filepath + 'Raw Data/Accounts.csv')['address']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OLWmVVF0w1J"
      },
      "source": [
        "###Create list of relevant accounts, and the starttime for the exercise\n",
        "#accounts = [19, 21, 25, 37, 44, 81, 90]\n",
        "accounts = list(range(ledger_data.shape[1]))\n",
        "start = 000\n",
        "end = 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxOhWATR6vN8"
      },
      "source": [
        "#Keep only the end price for bitcoin\n",
        "cleaned_data = pd.DataFrame(cleaned_data['end_price_bitcoin'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a65kVudR7Dxm"
      },
      "source": [
        "#Keep only the ledger data with the relevant numbers \n",
        "ledger_data = ledger_data.iloc[:, accounts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EP_pK_W-EgV"
      },
      "source": [
        "#Merge the datasets\n",
        "ledger_data.index.names = ['time']\n",
        "full_data = pd.merge(cleaned_data, ledger_data, on = \"time\", how = 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjbxtfFob8cB"
      },
      "source": [
        "#Create dataframe with the account name and the first id number (i.e. first trade)\n",
        "summary = pd.DataFrame({\"First_id\": full_data.reset_index(drop = True).ne(0).idxmax()}).reset_index(drop = True)\n",
        "summary2 = pd.DataFrame({\"Account_name\": ['No account']})\n",
        "summary3 = pd.DataFrame({\"Account_name\": account_names[accounts]})\n",
        "summary2 = summary2.append(summary3).reset_index(drop = True)\n",
        "summary = pd.merge(summary2, summary, left_index= True, right_index = True, how = 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pAoaa13ACdw"
      },
      "source": [
        "#Calculate percentage change, and shift price up one to match with previous period's transactions\n",
        "full_data = full_data.pct_change()\n",
        "full_data['end_price_bitcoin'] = full_data['end_price_bitcoin'].shift(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ6OyXQRAoXL"
      },
      "source": [
        "#Make each of the percentage changes in balance -1, 0 or 1\n",
        "full_data.loc[:, full_data.columns != 'end_price_bitcoin'] = np.where(full_data.loc[:, full_data.columns != 'end_price_bitcoin']>0,1, np.where(full_data.loc[:, full_data.columns != 'end_price_bitcoin']<0,-1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxw6AiqxROSj"
      },
      "source": [
        "#Filter by the start and end time\n",
        "full_data = full_data.iloc[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBQNeaWNBme9"
      },
      "source": [
        "#Add 100 USD at the start\n",
        "full_data['Amount_Basic'] = 0\n",
        "full_data['Amount_Basic'].iloc[0] = 100\n",
        "for i in range(len(accounts)):\n",
        "  full_data['Amount_Copying_' + str(accounts[i])] = 0\n",
        "  full_data['Amount_Copying_' + str(accounts[i])].iloc[0] = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUaxbz7KCqmk"
      },
      "source": [
        "#Calculate the basic amount over time\n",
        "#Loop upwards, calculating the next row each time\n",
        "for i in range(full_data.shape[0]-1):\n",
        "  #Calculate the new value\n",
        "  row_number_temp = i + 1\n",
        "  full_data['Amount_Basic'].iloc[row_number_temp] = full_data['Amount_Basic'].iloc[row_number_temp-1] * (1 + (full_data['end_price_bitcoin'].iloc[row_number_temp-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcdWajxxAZpF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f17aa0b6-d25c-45bc-8890-12f17dd176b6"
      },
      "source": [
        "#Calculate the copying amounts over time\n",
        "#Loop upwards, calculating the next row each time\n",
        "for j in range(len(accounts)):\n",
        "  if j % 5 == 0:\n",
        "    print(j)\n",
        "  for i in range(full_data.shape[0]-1):\n",
        "    row_number_temp = i + 1\n",
        "    #Calculate the new value\n",
        "    if i == 0:\n",
        "      #Start by having bitcoin, unless it says sell\n",
        "      full_data['Amount_Copying_' + str(accounts[j])].iloc[row_number_temp] = full_data['Amount_Copying_' + str(accounts[j])].iloc[row_number_temp-1] * np.where(full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp] in [0,1], 1 + full_data['end_price_bitcoin'].iloc[row_number_temp-1], 1)\n",
        "      full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp]  = np.where(full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp] in [0,1], 1, -1)\n",
        "    else:\n",
        "      #For holding previously\n",
        "      if full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp-1] == 1:\n",
        "        #If 0 or 1, then keep (and change prediction to 1). If -1, then sell (and keep prediction as -1)\n",
        "        full_data['Amount_Copying_' + str(accounts[j])].iloc[row_number_temp] = full_data['Amount_Copying_' + str(accounts[j])].iloc[row_number_temp-1] * np.where(full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp] in [0,1], 1 + full_data['end_price_bitcoin'].iloc[row_number_temp-1], 1)\n",
        "        full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp]  = np.where(full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp] in [0,1], 1, -1)\n",
        "      #For not holding previously\n",
        "      if full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp-1] == -1:\n",
        "        #If -1 or 0, then don't buy (and change prediction to -1). If 1, then buy (and keep prediction as 1)\n",
        "        full_data['Amount_Copying_' + str(accounts[j])].iloc[row_number_temp] = full_data['Amount_Copying_' + str(accounts[j])].iloc[row_number_temp-1] * np.where(full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp] == 1, 1 + full_data['end_price_bitcoin'].iloc[row_number_temp-1], 1)\n",
        "        full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp]  = np.where(full_data['balance_' + str(accounts[j]+1)].iloc[row_number_temp] in [-1,0], -1, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "50\n",
            "55\n",
            "60\n",
            "65\n",
            "70\n",
            "75\n",
            "80\n",
            "85\n",
            "90\n",
            "95\n",
            "100\n",
            "105\n",
            "110\n",
            "115\n",
            "120\n",
            "125\n",
            "130\n",
            "135\n",
            "140\n",
            "145\n",
            "150\n",
            "155\n",
            "160\n",
            "165\n",
            "170\n",
            "175\n",
            "180\n",
            "185\n",
            "190\n",
            "195\n",
            "200\n",
            "205\n",
            "210\n",
            "215\n",
            "220\n",
            "225\n",
            "230\n",
            "235\n",
            "240\n",
            "245\n",
            "250\n",
            "255\n",
            "260\n",
            "265\n",
            "270\n",
            "275\n",
            "280\n",
            "285\n",
            "290\n",
            "295\n",
            "300\n",
            "305\n",
            "310\n",
            "315\n",
            "320\n",
            "325\n",
            "330\n",
            "335\n",
            "340\n",
            "345\n",
            "350\n",
            "355\n",
            "360\n",
            "365\n",
            "370\n",
            "375\n",
            "380\n",
            "385\n",
            "390\n",
            "395\n",
            "400\n",
            "405\n",
            "410\n",
            "415\n",
            "420\n",
            "425\n",
            "430\n",
            "435\n",
            "440\n",
            "445\n",
            "450\n",
            "455\n",
            "460\n",
            "465\n",
            "470\n",
            "475\n",
            "480\n",
            "485\n",
            "490\n",
            "495\n",
            "500\n",
            "505\n",
            "510\n",
            "515\n",
            "520\n",
            "525\n",
            "530\n",
            "535\n",
            "540\n",
            "545\n",
            "550\n",
            "555\n",
            "560\n",
            "565\n",
            "570\n",
            "575\n",
            "580\n",
            "585\n",
            "590\n",
            "595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfGx9NMBefnk"
      },
      "source": [
        "###Save details in a dataframe, looking at starting at different points in time\n",
        "#Create list of times\n",
        "times = list(range(0,41000,250))\n",
        "\n",
        "#Loop through these, summarise, and merge with summary data\n",
        "for i in range(len(times)):\n",
        "  full_data_temp = full_data[full_data.columns[pd.Series(full_data.columns).str.startswith('Amount')]].iloc[times[i]:].reset_index(drop = True)\n",
        "  full_data_temp.iloc[-1] = 100*full_data_temp.iloc[-1]/full_data_temp.iloc[0]\n",
        "  summary_2 = pd.DataFrame({\"Start_\" + str(times[i]) :full_data_temp[full_data_temp.columns[pd.Series(full_data_temp.columns).str.startswith('Amount')]].reset_index(drop = True).iloc[-1]}).reset_index(drop=True)\n",
        "  summary = pd.merge(summary, summary_2, left_index = True, right_index = True, how = 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBQ7-Z-5la27"
      },
      "source": [
        "###Do the same, but for starting and ending\n",
        "#Create list of times\n",
        "times_start = list(range(0,41000,250))\n",
        "times_end = list(range(250,41250,250))\n",
        "\n",
        "#Loop through these, summarise, and merge with summary data\n",
        "for i in range(len(times_start)):\n",
        "  full_data_temp = full_data[full_data.columns[pd.Series(full_data.columns).str.startswith('Amount')]].iloc[times_start[i]:times_end[i]].reset_index(drop = True)\n",
        "  full_data_temp.iloc[-1] = 100*full_data_temp.iloc[-1]/full_data_temp.iloc[0]\n",
        "  summary_2 = pd.DataFrame({\"Start_End_\" + str(times_start[i]) :full_data_temp[full_data_temp.columns[pd.Series(full_data_temp.columns).str.startswith('Amount')]].reset_index(drop = True).iloc[-1]}).reset_index(drop=True)\n",
        "  summary = pd.merge(summary, summary_2, left_index = True, right_index = True, how = 'inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgYgVetbi2K6"
      },
      "source": [
        "### Add the account ID\n",
        "summary3 = pd.DataFrame({\"Account_ID\": [-99]})\n",
        "summary2 = pd.DataFrame({\"Account_ID\": accounts})\n",
        "summary2 = summary3.append(summary2).reset_index()\n",
        "summary = pd.merge(summary2, summary, left_index = True, right_index = True, how = 'inner')\n",
        "summary.drop(labels = \"index\", axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-81IfE4MceE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "656637be-2909-4642-b4c1-adba92d010fd"
      },
      "source": [
        "#print results for total\n",
        "print(\"Leaving in: \" + str(full_data['Amount_Basic'].iloc[-1]))\n",
        "for j in range(len(accounts)):\n",
        "  print(\"Account \" + str(accounts[j]) + \": \" + str(full_data['Amount_Copying_' + str(accounts[j])].iloc[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Leaving in: 3207.723450638447\n",
            "Account 0: 4113.546703758477\n",
            "Account 1: 2997.9721266843517\n",
            "Account 2: 3207.723450638447\n",
            "Account 3: 3207.723450638447\n",
            "Account 4: 3207.723450638447\n",
            "Account 5: 3207.723450638447\n",
            "Account 6: 3207.723450638447\n",
            "Account 7: 3384.941610727739\n",
            "Account 8: 3230.0616880377443\n",
            "Account 9: 2243.6574531718265\n",
            "Account 10: 3207.723450638447\n",
            "Account 11: 2996.225820093803\n",
            "Account 12: 3207.723450638447\n",
            "Account 13: 3449.9390991006594\n",
            "Account 14: 2297.756278340381\n",
            "Account 15: 3506.166554982679\n",
            "Account 16: 3207.723450638447\n",
            "Account 17: 3207.723450638447\n",
            "Account 18: 3207.723450638447\n",
            "Account 19: 566.4775590098149\n",
            "Account 20: 3207.723450638447\n",
            "Account 21: 3580.8196698293696\n",
            "Account 22: 3207.723450638447\n",
            "Account 23: 3207.723450638447\n",
            "Account 24: 3207.723450638447\n",
            "Account 25: 4155.311767434355\n",
            "Account 26: 3853.287964275811\n",
            "Account 27: 2899.823614226554\n",
            "Account 28: 3207.723450638447\n",
            "Account 29: 3207.723450638447\n",
            "Account 30: 946.815380451089\n",
            "Account 31: 3207.723450638447\n",
            "Account 32: 1035.5220109276477\n",
            "Account 33: 3207.723450638447\n",
            "Account 34: 3207.723450638447\n",
            "Account 35: 1591.4746642804562\n",
            "Account 36: 3207.723450638447\n",
            "Account 37: 15421.116461253054\n",
            "Account 38: 3207.723450638447\n",
            "Account 39: 3749.0434080203727\n",
            "Account 40: 3207.723450638447\n",
            "Account 41: 3207.723450638447\n",
            "Account 42: 5258.242088902088\n",
            "Account 43: 3207.723450638447\n",
            "Account 44: 2537.4961463770637\n",
            "Account 45: 6895.011248639187\n",
            "Account 46: 3858.3637477405064\n",
            "Account 47: 3394.4921437620883\n",
            "Account 48: 3207.723450638447\n",
            "Account 49: 3207.723450638447\n",
            "Account 50: 3207.723450638447\n",
            "Account 51: 3204.9078940046475\n",
            "Account 52: 3207.723450638447\n",
            "Account 53: 3207.723450638447\n",
            "Account 54: 2834.63598596389\n",
            "Account 55: 3207.723450638447\n",
            "Account 56: 3207.723450638447\n",
            "Account 57: 3207.723450638447\n",
            "Account 58: 3207.723450638447\n",
            "Account 59: 2585.5768934970974\n",
            "Account 60: 3207.723450638447\n",
            "Account 61: 3207.723450638447\n",
            "Account 62: 3207.723450638447\n",
            "Account 63: 3207.723450638447\n",
            "Account 64: 3207.723450638447\n",
            "Account 65: 3207.723450638447\n",
            "Account 66: 3207.723450638447\n",
            "Account 67: 3207.723450638447\n",
            "Account 68: 3207.723450638447\n",
            "Account 69: 3207.723450638447\n",
            "Account 70: 3207.723450638447\n",
            "Account 71: 3207.723450638447\n",
            "Account 72: 3207.723450638447\n",
            "Account 73: 3207.723450638447\n",
            "Account 74: 3207.723450638447\n",
            "Account 75: 3207.723450638447\n",
            "Account 76: 3207.723450638447\n",
            "Account 77: 3207.723450638447\n",
            "Account 78: 3207.723450638447\n",
            "Account 79: 1713.4726453998671\n",
            "Account 80: 3207.723450638447\n",
            "Account 81: 3044.229060539313\n",
            "Account 82: 3278.7828205288424\n",
            "Account 83: 3207.723450638447\n",
            "Account 84: 8286.213445934243\n",
            "Account 85: 1159.3068939311368\n",
            "Account 86: 3768.538180592367\n",
            "Account 87: 3207.723450638447\n",
            "Account 88: 3207.723450638447\n",
            "Account 89: 3120.5169153481147\n",
            "Account 90: 3207.723450638447\n",
            "Account 91: 3921.006372613901\n",
            "Account 92: 3207.723450638447\n",
            "Account 93: 3207.723450638447\n",
            "Account 94: 3207.723450638447\n",
            "Account 95: 3340.6978196719056\n",
            "Account 96: 3207.723450638447\n",
            "Account 97: 3207.723450638447\n",
            "Account 98: 3207.723450638447\n",
            "Account 99: 3207.723450638447\n",
            "Account 100: 3207.723450638447\n",
            "Account 101: 3207.723450638447\n",
            "Account 102: 3207.723450638447\n",
            "Account 103: 3207.723450638447\n",
            "Account 104: 3207.723450638447\n",
            "Account 105: 3207.723450638447\n",
            "Account 106: 1356.729949897215\n",
            "Account 107: 1753.2931393824235\n",
            "Account 108: 3207.723450638447\n",
            "Account 109: 3207.723450638447\n",
            "Account 110: 3207.723450638447\n",
            "Account 111: 3207.723450638447\n",
            "Account 112: 3207.723450638447\n",
            "Account 113: 3207.723450638447\n",
            "Account 114: 2307.853946946264\n",
            "Account 115: 1217.3805047228336\n",
            "Account 116: 3207.723450638447\n",
            "Account 117: 3207.723450638447\n",
            "Account 118: 3054.0377614416775\n",
            "Account 119: 3511.5120708361187\n",
            "Account 120: 3207.723450638447\n",
            "Account 121: 3939.9487139138078\n",
            "Account 122: 3207.723450638447\n",
            "Account 123: 3207.723450638447\n",
            "Account 124: 3207.723450638447\n",
            "Account 125: 3207.723450638447\n",
            "Account 126: 3207.723450638447\n",
            "Account 127: 3207.723450638447\n",
            "Account 128: 3207.723450638447\n",
            "Account 129: 3207.723450638447\n",
            "Account 130: 4193.1900843488875\n",
            "Account 131: 3207.723450638447\n",
            "Account 132: 3207.723450638447\n",
            "Account 133: 3207.723450638447\n",
            "Account 134: 3207.723450638447\n",
            "Account 135: 2298.753413547205\n",
            "Account 136: 3516.739333540988\n",
            "Account 137: 3207.723450638447\n",
            "Account 138: 3578.9798411931847\n",
            "Account 139: 3207.723450638447\n",
            "Account 140: 2435.6400819524447\n",
            "Account 141: 3207.723450638447\n",
            "Account 142: 3207.723450638447\n",
            "Account 143: 3207.723450638447\n",
            "Account 144: 3207.723450638447\n",
            "Account 145: 3207.723450638447\n",
            "Account 146: 3207.723450638447\n",
            "Account 147: 3207.723450638447\n",
            "Account 148: 3207.723450638447\n",
            "Account 149: 3207.723450638447\n",
            "Account 150: 3207.723450638447\n",
            "Account 151: 3207.723450638447\n",
            "Account 152: 3207.723450638447\n",
            "Account 153: 3207.723450638447\n",
            "Account 154: 3207.723450638447\n",
            "Account 155: 3207.723450638447\n",
            "Account 156: 3207.723450638447\n",
            "Account 157: 3207.723450638447\n",
            "Account 158: 3207.723450638447\n",
            "Account 159: 3207.723450638447\n",
            "Account 160: 3207.723450638447\n",
            "Account 161: 3207.723450638447\n",
            "Account 162: 3207.723450638447\n",
            "Account 163: 3207.723450638447\n",
            "Account 164: 3207.723450638447\n",
            "Account 165: 3207.723450638447\n",
            "Account 166: 3207.723450638447\n",
            "Account 167: 3207.723450638447\n",
            "Account 168: 3207.723450638447\n",
            "Account 169: 3207.723450638447\n",
            "Account 170: 3207.723450638447\n",
            "Account 171: 3207.723450638447\n",
            "Account 172: 3207.723450638447\n",
            "Account 173: 3207.723450638447\n",
            "Account 174: 3207.723450638447\n",
            "Account 175: 3207.723450638447\n",
            "Account 176: 3207.723450638447\n",
            "Account 177: 3207.723450638447\n",
            "Account 178: 3207.723450638447\n",
            "Account 179: 3207.723450638447\n",
            "Account 180: 3207.723450638447\n",
            "Account 181: 3207.723450638447\n",
            "Account 182: 3207.723450638447\n",
            "Account 183: 3207.723450638447\n",
            "Account 184: 3207.723450638447\n",
            "Account 185: 3207.723450638447\n",
            "Account 186: 3207.723450638447\n",
            "Account 187: 3207.723450638447\n",
            "Account 188: 3207.723450638447\n",
            "Account 189: 3207.723450638447\n",
            "Account 190: 3207.723450638447\n",
            "Account 191: 3207.723450638447\n",
            "Account 192: 3207.723450638447\n",
            "Account 193: 3207.723450638447\n",
            "Account 194: 3207.723450638447\n",
            "Account 195: 3207.723450638447\n",
            "Account 196: 3207.723450638447\n",
            "Account 197: 3207.723450638447\n",
            "Account 198: 3207.723450638447\n",
            "Account 199: 3207.723450638447\n",
            "Account 200: 3207.723450638447\n",
            "Account 201: 3207.723450638447\n",
            "Account 202: 3207.723450638447\n",
            "Account 203: 3207.723450638447\n",
            "Account 204: 3207.723450638447\n",
            "Account 205: 3207.723450638447\n",
            "Account 206: 3207.723450638447\n",
            "Account 207: 3207.723450638447\n",
            "Account 208: 3207.723450638447\n",
            "Account 209: 3207.723450638447\n",
            "Account 210: 3207.723450638447\n",
            "Account 211: 3207.723450638447\n",
            "Account 212: 3207.723450638447\n",
            "Account 213: 3207.723450638447\n",
            "Account 214: 3207.723450638447\n",
            "Account 215: 3207.723450638447\n",
            "Account 216: 3207.723450638447\n",
            "Account 217: 3207.723450638447\n",
            "Account 218: 3207.723450638447\n",
            "Account 219: 3207.723450638447\n",
            "Account 220: 3207.723450638447\n",
            "Account 221: 3207.723450638447\n",
            "Account 222: 3207.723450638447\n",
            "Account 223: 3207.723450638447\n",
            "Account 224: 3207.723450638447\n",
            "Account 225: 3207.723450638447\n",
            "Account 226: 3207.723450638447\n",
            "Account 227: 3207.723450638447\n",
            "Account 228: 3207.723450638447\n",
            "Account 229: 2362.824879455122\n",
            "Account 230: 3207.723450638447\n",
            "Account 231: 3207.723450638447\n",
            "Account 232: 3207.723450638447\n",
            "Account 233: 3207.723450638447\n",
            "Account 234: 3207.723450638447\n",
            "Account 235: 3814.0691979187427\n",
            "Account 236: 3167.3544067269045\n",
            "Account 237: 2522.9212736036393\n",
            "Account 238: 3207.723450638447\n",
            "Account 239: 3207.723450638447\n",
            "Account 240: 3207.723450638447\n",
            "Account 241: 3207.723450638447\n",
            "Account 242: 640.8283420750145\n",
            "Account 243: 3207.723450638447\n",
            "Account 244: 3207.723450638447\n",
            "Account 245: 2217.0274057925953\n",
            "Account 246: 3207.723450638447\n",
            "Account 247: 3207.723450638447\n",
            "Account 248: 3207.723450638447\n",
            "Account 249: 3207.723450638447\n",
            "Account 250: 3207.723450638447\n",
            "Account 251: 3207.723450638447\n",
            "Account 252: 3207.723450638447\n",
            "Account 253: 3207.723450638447\n",
            "Account 254: 3207.723450638447\n",
            "Account 255: 3373.1495963594934\n",
            "Account 256: 3207.723450638447\n",
            "Account 257: 5527.603268159314\n",
            "Account 258: 186.78366073829162\n",
            "Account 259: 3180.9463925605587\n",
            "Account 260: 1071.21978454275\n",
            "Account 261: 1379.6850606470468\n",
            "Account 262: 3207.723450638447\n",
            "Account 263: 3207.723450638447\n",
            "Account 264: 3207.723450638447\n",
            "Account 265: 5232.154084898455\n",
            "Account 266: 3148.8808291567266\n",
            "Account 267: 6345.1995421431275\n",
            "Account 268: 3207.723450638447\n",
            "Account 269: 3207.723450638447\n",
            "Account 270: 3207.723450638447\n",
            "Account 271: 3207.723450638447\n",
            "Account 272: 3207.723450638447\n",
            "Account 273: 3207.723450638447\n",
            "Account 274: 4920.365543609291\n",
            "Account 275: 3207.723450638447\n",
            "Account 276: 3207.723450638447\n",
            "Account 277: 3207.723450638447\n",
            "Account 278: 3207.723450638447\n",
            "Account 279: 3207.723450638447\n",
            "Account 280: 3207.723450638447\n",
            "Account 281: 3207.723450638447\n",
            "Account 282: 3207.723450638447\n",
            "Account 283: 3207.723450638447\n",
            "Account 284: 3690.0553116268597\n",
            "Account 285: 3207.723450638447\n",
            "Account 286: 3207.723450638447\n",
            "Account 287: 3207.723450638447\n",
            "Account 288: 3207.723450638447\n",
            "Account 289: 450.10873827268387\n",
            "Account 290: 3207.723450638447\n",
            "Account 291: 2182.9112762393534\n",
            "Account 292: 3207.723450638447\n",
            "Account 293: 3845.4309825558507\n",
            "Account 294: 3207.723450638447\n",
            "Account 295: 3207.723450638447\n",
            "Account 296: 3207.723450638447\n",
            "Account 297: 2424.3013606356258\n",
            "Account 298: 188.9953228441082\n",
            "Account 299: 3207.723450638447\n",
            "Account 300: 3207.723450638447\n",
            "Account 301: 3207.723450638447\n",
            "Account 302: 3207.723450638447\n",
            "Account 303: 3207.723450638447\n",
            "Account 304: 3207.723450638447\n",
            "Account 305: 3207.723450638447\n",
            "Account 306: 3207.723450638447\n",
            "Account 307: 3207.723450638447\n",
            "Account 308: 3509.8656388982413\n",
            "Account 309: 3207.723450638447\n",
            "Account 310: 3207.723450638447\n",
            "Account 311: 3207.723450638447\n",
            "Account 312: 3207.723450638447\n",
            "Account 313: 3207.723450638447\n",
            "Account 314: 3207.723450638447\n",
            "Account 315: 3207.723450638447\n",
            "Account 316: 3207.723450638447\n",
            "Account 317: 3208.4643911072717\n",
            "Account 318: 3207.723450638447\n",
            "Account 319: 3207.723450638447\n",
            "Account 320: 3207.723450638447\n",
            "Account 321: 3207.723450638447\n",
            "Account 322: 3207.723450638447\n",
            "Account 323: 1776.5173362491817\n",
            "Account 324: 3207.723450638447\n",
            "Account 325: 3207.723450638447\n",
            "Account 326: 3207.723450638447\n",
            "Account 327: 3207.723450638447\n",
            "Account 328: 3207.723450638447\n",
            "Account 329: 3207.723450638447\n",
            "Account 330: 3207.723450638447\n",
            "Account 331: 2653.4683444367097\n",
            "Account 332: 2884.707882597023\n",
            "Account 333: 3207.723450638447\n",
            "Account 334: 1906.6778786460984\n",
            "Account 335: 3207.723450638447\n",
            "Account 336: 3207.723450638447\n",
            "Account 337: 3207.723450638447\n",
            "Account 338: 2812.012382808571\n",
            "Account 339: 3207.723450638447\n",
            "Account 340: 3420.0241062028085\n",
            "Account 341: 3207.723450638447\n",
            "Account 342: 3207.723450638447\n",
            "Account 343: 3084.9608135713215\n",
            "Account 344: 3207.723450638447\n",
            "Account 345: 3207.723450638447\n",
            "Account 346: 3207.723450638447\n",
            "Account 347: 3207.723450638447\n",
            "Account 348: 3207.723450638447\n",
            "Account 349: 3207.723450638447\n",
            "Account 350: 3207.723450638447\n",
            "Account 351: 3207.723450638447\n",
            "Account 352: 3207.723450638447\n",
            "Account 353: 3207.723450638447\n",
            "Account 354: 3207.723450638447\n",
            "Account 355: 3207.723450638447\n",
            "Account 356: 3207.723450638447\n",
            "Account 357: 5641.2593264883135\n",
            "Account 358: 3207.723450638447\n",
            "Account 359: 3207.723450638447\n",
            "Account 360: 3207.723450638447\n",
            "Account 361: 3207.723450638447\n",
            "Account 362: 3207.723450638447\n",
            "Account 363: 3207.723450638447\n",
            "Account 364: 3207.723450638447\n",
            "Account 365: 4655.130361801617\n",
            "Account 366: 3207.723450638447\n",
            "Account 367: 3207.723450638447\n",
            "Account 368: 3207.723450638447\n",
            "Account 369: 3207.723450638447\n",
            "Account 370: 3207.723450638447\n",
            "Account 371: 2648.9301749909896\n",
            "Account 372: 3207.723450638447\n",
            "Account 373: 2656.4039013566403\n",
            "Account 374: 3207.723450638447\n",
            "Account 375: 3207.723450638447\n",
            "Account 376: 4089.01497707712\n",
            "Account 377: 3207.723450638447\n",
            "Account 378: 3207.723450638447\n",
            "Account 379: 3207.723450638447\n",
            "Account 380: 3207.723450638447\n",
            "Account 381: 3207.723450638447\n",
            "Account 382: 3207.723450638447\n",
            "Account 383: 3207.723450638447\n",
            "Account 384: 2733.221737776398\n",
            "Account 385: 2680.6680161943364\n",
            "Account 386: 3207.723450638447\n",
            "Account 387: 3207.723450638447\n",
            "Account 388: 3207.723450638447\n",
            "Account 389: 3207.723450638447\n",
            "Account 390: 3207.723450638447\n",
            "Account 391: 3207.723450638447\n",
            "Account 392: 3207.723450638447\n",
            "Account 393: 3207.723450638447\n",
            "Account 394: 3207.723450638447\n",
            "Account 395: 3462.2329718043616\n",
            "Account 396: 5280.237488544395\n",
            "Account 397: 3207.723450638447\n",
            "Account 398: 3207.723450638447\n",
            "Account 399: 3207.723450638447\n",
            "Account 400: 3207.723450638447\n",
            "Account 401: 3192.794827959383\n",
            "Account 402: 3207.723450638447\n",
            "Account 403: 3207.723450638447\n",
            "Account 404: 3202.5022036779555\n",
            "Account 405: 3207.723450638447\n",
            "Account 406: 3207.723450638447\n",
            "Account 407: 3207.723450638447\n",
            "Account 408: 3207.723450638447\n",
            "Account 409: 4393.575563986675\n",
            "Account 410: 3207.723450638447\n",
            "Account 411: 3207.723450638447\n",
            "Account 412: 3207.723450638447\n",
            "Account 413: 3207.723450638447\n",
            "Account 414: 3207.723450638447\n",
            "Account 415: 3207.723450638447\n",
            "Account 416: 3207.723450638447\n",
            "Account 417: 3207.723450638447\n",
            "Account 418: 3207.723450638447\n",
            "Account 419: 3207.723450638447\n",
            "Account 420: 3207.723450638447\n",
            "Account 421: 3207.723450638447\n",
            "Account 422: 3207.723450638447\n",
            "Account 423: 3207.723450638447\n",
            "Account 424: 3207.723450638447\n",
            "Account 425: 3207.723450638447\n",
            "Account 426: 304.3287855321408\n",
            "Account 427: 3207.723450638447\n",
            "Account 428: 3207.723450638447\n",
            "Account 429: 3207.723450638447\n",
            "Account 430: 2532.102300766026\n",
            "Account 431: 4081.3270318661207\n",
            "Account 432: 3207.723450638447\n",
            "Account 433: 3207.723450638447\n",
            "Account 434: 2699.6528272751275\n",
            "Account 435: 3207.723450638447\n",
            "Account 436: 3207.723450638447\n",
            "Account 437: 3207.723450638447\n",
            "Account 438: 3207.723450638447\n",
            "Account 439: 3207.723450638447\n",
            "Account 440: 3207.723450638447\n",
            "Account 441: 3911.8735559439324\n",
            "Account 442: 3207.723450638447\n",
            "Account 443: 1377.5789017296854\n",
            "Account 444: 3207.723450638447\n",
            "Account 445: 3207.723450638447\n",
            "Account 446: 3207.723450638447\n",
            "Account 447: 3207.723450638447\n",
            "Account 448: 3207.723450638447\n",
            "Account 449: 4128.81630591308\n",
            "Account 450: 3207.723450638447\n",
            "Account 451: 4338.435770576085\n",
            "Account 452: 3231.3619587632215\n",
            "Account 453: 3207.723450638447\n",
            "Account 454: 3207.723450638447\n",
            "Account 455: 3207.723450638447\n",
            "Account 456: 3207.723450638447\n",
            "Account 457: 3207.723450638447\n",
            "Account 458: 3207.723450638447\n",
            "Account 459: 3207.723450638447\n",
            "Account 460: 3207.723450638447\n",
            "Account 461: 3207.723450638447\n",
            "Account 462: 3207.723450638447\n",
            "Account 463: 3207.723450638447\n",
            "Account 464: 3207.723450638447\n",
            "Account 465: 3207.723450638447\n",
            "Account 466: 3207.723450638447\n",
            "Account 467: 3207.723450638447\n",
            "Account 468: 3207.723450638447\n",
            "Account 469: 2711.617667836755\n",
            "Account 470: 3207.723450638447\n",
            "Account 471: 3207.723450638447\n",
            "Account 472: 3207.723450638447\n",
            "Account 473: 3207.723450638447\n",
            "Account 474: 3207.723450638447\n",
            "Account 475: 3207.723450638447\n",
            "Account 476: 3207.723450638447\n",
            "Account 477: 3207.723450638447\n",
            "Account 478: 3207.723450638447\n",
            "Account 479: 3207.723450638447\n",
            "Account 480: 3207.723450638447\n",
            "Account 481: 3207.723450638447\n",
            "Account 482: 3287.181877770306\n",
            "Account 483: 3207.723450638447\n",
            "Account 484: 3207.723450638447\n",
            "Account 485: 3207.723450638447\n",
            "Account 486: 3207.723450638447\n",
            "Account 487: 3207.723450638447\n",
            "Account 488: 3207.723450638447\n",
            "Account 489: 6312.679661080225\n",
            "Account 490: 3207.723450638447\n",
            "Account 491: 3207.723450638447\n",
            "Account 492: 3207.723450638447\n",
            "Account 493: 3207.723450638447\n",
            "Account 494: 3496.573168805314\n",
            "Account 495: 3207.723450638447\n",
            "Account 496: 3207.723450638447\n",
            "Account 497: 3207.723450638447\n",
            "Account 498: 3207.723450638447\n",
            "Account 499: 2626.290356873447\n",
            "Account 500: 3207.723450638447\n",
            "Account 501: 3207.723450638447\n",
            "Account 502: 4620.228540589607\n",
            "Account 503: 3207.723450638447\n",
            "Account 504: 2480.2018884937374\n",
            "Account 505: 2449.665213329191\n",
            "Account 506: 3207.723450638447\n",
            "Account 507: 3207.723450638447\n",
            "Account 508: 3207.723450638447\n",
            "Account 509: 3207.723450638447\n",
            "Account 510: 3207.723450638447\n",
            "Account 511: 1224.922331111176\n",
            "Account 512: 3207.723450638447\n",
            "Account 513: 3207.723450638447\n",
            "Account 514: 3220.110254581167\n",
            "Account 515: 3207.723450638447\n",
            "Account 516: 3207.723450638447\n",
            "Account 517: 3207.723450638447\n",
            "Account 518: 3207.723450638447\n",
            "Account 519: 6137.591214884029\n",
            "Account 520: 3207.723450638447\n",
            "Account 521: 2565.6464579358576\n",
            "Account 522: 1997.8540929446515\n",
            "Account 523: 3207.723450638447\n",
            "Account 524: 2692.556637761177\n",
            "Account 525: 3207.723450638447\n",
            "Account 526: 2127.809322288138\n",
            "Account 527: 3207.723450638447\n",
            "Account 528: 3207.723450638447\n",
            "Account 529: 3207.723450638447\n",
            "Account 530: 3207.723450638447\n",
            "Account 531: 4163.382171579428\n",
            "Account 532: 3207.723450638447\n",
            "Account 533: 3207.723450638447\n",
            "Account 534: 3207.723450638447\n",
            "Account 535: 3207.723450638447\n",
            "Account 536: 3207.723450638447\n",
            "Account 537: 136.99681898916165\n",
            "Account 538: 3207.723450638447\n",
            "Account 539: 3207.723450638447\n",
            "Account 540: 3207.723450638447\n",
            "Account 541: 3207.723450638447\n",
            "Account 542: 3047.0816508068897\n",
            "Account 543: 3207.723450638447\n",
            "Account 544: 3207.723450638447\n",
            "Account 545: 1712.7544394506474\n",
            "Account 546: 2675.9634238526687\n",
            "Account 547: 3207.723450638447\n",
            "Account 548: 3207.723450638447\n",
            "Account 549: 3207.723450638447\n",
            "Account 550: 3207.723450638447\n",
            "Account 551: 2564.6064918310203\n",
            "Account 552: 3207.723450638447\n",
            "Account 553: 3213.643955853631\n",
            "Account 554: 3207.723450638447\n",
            "Account 555: 6387.340273564468\n",
            "Account 556: 3207.723450638447\n",
            "Account 557: 3207.723450638447\n",
            "Account 558: 3207.723450638447\n",
            "Account 559: 3207.723450638447\n",
            "Account 560: 3207.723450638447\n",
            "Account 561: 3207.723450638447\n",
            "Account 562: 4261.522890065441\n",
            "Account 563: 2537.799750856434\n",
            "Account 564: 3207.723450638447\n",
            "Account 565: 3207.723450638447\n",
            "Account 566: 3207.723450638447\n",
            "Account 567: 3207.723450638447\n",
            "Account 568: 3207.723450638447\n",
            "Account 569: 3207.723450638447\n",
            "Account 570: 3207.723450638447\n",
            "Account 571: 3207.723450638447\n",
            "Account 572: 3207.723450638447\n",
            "Account 573: 3207.723450638447\n",
            "Account 574: 3207.723450638447\n",
            "Account 575: 3207.723450638447\n",
            "Account 576: 3207.723450638447\n",
            "Account 577: 3207.723450638447\n",
            "Account 578: 3207.723450638447\n",
            "Account 579: 3207.723450638447\n",
            "Account 580: 3207.723450638447\n",
            "Account 581: 3207.723450638447\n",
            "Account 582: 3207.723450638447\n",
            "Account 583: 3207.723450638447\n",
            "Account 584: 3207.723450638447\n",
            "Account 585: 3207.723450638447\n",
            "Account 586: 3207.723450638447\n",
            "Account 587: 3207.723450638447\n",
            "Account 588: 3207.723450638447\n",
            "Account 589: 3207.723450638447\n",
            "Account 590: 3207.723450638447\n",
            "Account 591: 3207.723450638447\n",
            "Account 592: 3207.723450638447\n",
            "Account 593: 3207.723450638447\n",
            "Account 594: 3207.723450638447\n",
            "Account 595: 3207.723450638447\n",
            "Account 596: 3207.723450638447\n",
            "Account 597: 3207.723450638447\n",
            "Account 598: 3207.723450638447\n",
            "Account 599: 3207.723450638447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNGZ6Pt8LDyk"
      },
      "source": [
        "#Plot this\n",
        "#plotting_data = full_data[full_data.columns[pd.Series(full_data.columns).str.startswith('Amount')]].reset_index(drop = True).reset_index()\n",
        "#plotting_data = plotting_data.iloc[:]\n",
        "#plotting_data = pd.melt(plotting_data, ['index'])\n",
        "#ax = sns.lineplot(x = 'index', y=\"value\", hue = 'variable', data=plotting_data, ci = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e-NnOc9iKFx"
      },
      "source": [
        "#Save all results for exploring\n",
        "summary.to_csv(my_filepath + 'Results/Results from copying.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}